{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Content for 03_Modeling Pipeline Setup.ipynb\n",
    "This notebook loads enriched data from 01_data_collection.ipynb, adds lagged features, prepares data, trains classical models (LR, RF, LogReg, KMeans), and LSTM models.\n",
    "Saves model results to model_results_dir."
   ],
   "id": "7c07cd426607dbde"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, mean_absolute_error, r2_score\n",
    "from sklearn.cluster import KMeans\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Input, Dropout, Bidirectional\n",
    "\n",
    "# Set paths (same as previous)\n",
    "ROOT = os.getcwd()\n",
    "DATA_DIR = os.path.join(ROOT, \"data\")\n",
    "ENRICHED_DIR = os.path.join(DATA_DIR, \"enriched\")\n",
    "MODEL_RESULTS_DIR = os.path.join(DATA_DIR, 'model_results')\n",
    "os.makedirs(MODEL_RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Commodity names\n",
    "commodities = [\"Gold\", \"WTI\", \"Wheat\", \"NaturalGas\", \"Copper\", \"Lithium\"]\n",
    "\n",
    "# Load enriched data\n",
    "merged_data = {}\n",
    "for name in commodities:\n",
    "    fname = f\"{name.lower()}_enriched.csv\"\n",
    "    path = os.path.join(ENRICHED_DIR, fname)\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path)\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        merged_data[name] = df\n",
    "    else:\n",
    "        print(f\"Missing enriched file for {name}\")"
   ],
   "id": "27187f41e14d8567"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Functions for modeling\n",
    "def prepare_features_targets(df, features, target):\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    return X, y\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def create_sequences(data, feature_cols, target_col, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[feature_cols].iloc[i:i+seq_length].values\n",
    "        y = data[target_col].iloc[i+seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "def add_lagged_features(df, feature_cols, max_lag=5):\n",
    "    for col in feature_cols:\n",
    "        for lag in range(1, max_lag + 1):\n",
    "            df[f'{col}_lag{lag}'] = df[col].shift(lag)\n",
    "    return df"
   ],
   "id": "fd9877f2d95e4e27"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Modeling\n",
    "target_return = 'Return'\n",
    "target_ma = 'MA_5'\n",
    "lag_features = ['GPRD', 'geo_keyword_hits']\n",
    "\n",
    "results = {}\n",
    "results_ma = {}\n",
    "\n",
    "for commodity, df in merged_data.items():\n",
    "    print(f\"Processing commodity: {commodity}\")\n",
    "\n",
    "    df['geo_keyword_hits'].fillna(0, inplace=True)\n",
    "    df['sentiment'].fillna(0, inplace=True)\n",
    "    if 'EVENT' in df.columns:\n",
    "        df['EVENT'].fillna('None', inplace=True)\n",
    "\n",
    "    # Add lagged features\n",
    "    df = add_lagged_features(df, lag_features, max_lag=5)\n",
    "    df.dropna(inplace=True)\n",
    "    print(f\"{commodity} - {len(df)} rows after lagged features added and NaN drops.\")\n",
    "\n",
    "    features_baseline = ['Return_lag1']\n",
    "    features_enhanced = ['Return_lag1', 'GPRD', 'geo_keyword_hits', 'sentiment'] + \\\n",
    "                        [f'{feat}_lag{lag}' for feat in lag_features for lag in range(1, 6)]\n",
    "\n",
    "    # Convert to numeric\n",
    "    df[features_enhanced] = df[features_enhanced].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Train/test split\n",
    "    split_date = pd.to_datetime('2000-01-01')  # Note: This date is early; adjust if needed for more train data\n",
    "    train_df = df[df['Date'] < split_date].copy()\n",
    "    test_df = df[df['Date'] >= split_date].copy()\n",
    "\n",
    "    print(f\"{commodity} - Train size: {len(train_df)}, Test size: {len(test_df)}\")\n",
    "    if len(train_df) < 20 or len(test_df) < 20:\n",
    "        print(f\"Insufficient samples for {commodity}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Classical models for Return\n",
    "    X_train_base, y_train = prepare_features_targets(train_df, features_baseline, target_return)\n",
    "    X_test_base, y_test = prepare_features_targets(test_df, features_baseline, target_return)\n",
    "    X_train_enh = train_df[features_enhanced]\n",
    "    X_test_enh = test_df[features_enhanced]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_enh_scaled = scaler.fit_transform(X_train_enh)\n",
    "    X_test_enh_scaled = scaler.transform(X_test_enh)\n",
    "\n",
    "    # Baseline LR\n",
    "    lr_base = LinearRegression()\n",
    "    lr_base.fit(X_train_base, y_train)\n",
    "    y_pred_base = lr_base.predict(X_test_base)\n",
    "    rmse_base = np.sqrt(mean_squared_error(y_test, y_pred_base))\n",
    "\n",
    "    # Enhanced RF\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train_enh_scaled, y_train)\n",
    "    y_pred_rf = rf.predict(X_test_enh_scaled)\n",
    "    rmse_enh = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "\n",
    "    results[commodity] = {'RMSE_baseline': rmse_base, 'RMSE_enhanced': rmse_enh}"
   ],
   "id": "f004711a1f71f295"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Classification\n",
    "    train_df['Return_binary'] = (train_df['Return'] > 0).astype(int)\n",
    "    test_df['Return_binary'] = (test_df['Return'] > 0).astype(int)\n",
    "\n",
    "    train_df = train_df.dropna(subset=features_enhanced)\n",
    "    test_df = test_df.dropna(subset=features_enhanced)\n",
    "\n",
    "    X_train_class = scaler.fit_transform(train_df[features_enhanced])\n",
    "    X_test_class = scaler.transform(test_df[features_enhanced])\n",
    "    y_train_class = train_df['Return_binary']\n",
    "    y_test_class = test_df['Return_binary']\n",
    "\n",
    "    logreg = LogisticRegression(max_iter=200)\n",
    "    logreg.fit(X_train_class, y_train_class)\n",
    "    y_pred_class = logreg.predict(X_test_class)\n",
    "    print(f\"{commodity} - Classification Accuracy: {accuracy_score(y_test_class, y_pred_class):.4f}\")\n",
    "\n",
    "    # Clustering\n",
    "    kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "    regime_features = df[['Vol_5', 'GPRD', 'geo_keyword_hits']].fillna(0)\n",
    "    df['Regime'] = kmeans.fit_predict(regime_features)"
   ],
   "id": "18736255400f12bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# LSTM for MA_5\n",
    "    y_train_ma = train_df[target_ma]\n",
    "    y_test_ma = test_df[target_ma]\n",
    "\n",
    "    minmax_scaler = MinMaxScaler()\n",
    "    train_features_scaled = minmax_scaler.fit_transform(X_train_enh)\n",
    "    test_features_scaled = minmax_scaler.transform(X_test_enh)\n",
    "\n",
    "    train_scaled_df = pd.DataFrame(train_features_scaled, columns=features_enhanced, index=train_df.index)\n",
    "    test_scaled_df = pd.DataFrame(test_features_scaled, columns=features_enhanced, index=test_df.index)\n",
    "\n",
    "    train_scaled_df[target_ma] = y_train_ma.values  # Align lengths\n",
    "    test_scaled_df[target_ma] = y_test_ma.values\n",
    "\n",
    "    train_scaled_df.dropna(subset=features_enhanced + [target_ma], inplace=True)\n",
    "    test_scaled_df.dropna(subset=features_enhanced + [target_ma], inplace=True)\n",
    "\n",
    "    seq_length = 10\n",
    "    X_train_seq, y_train_seq = create_sequences(train_scaled_df, features_enhanced, target_ma, seq_length)\n",
    "    X_test_seq, y_test_seq = create_sequences(test_scaled_df, features_enhanced, target_ma, seq_length)\n",
    "\n",
    "    if len(X_train_seq) == 0 or len(X_test_seq) == 0:\n",
    "        print(f\"Skipping LSTM for {commodity} due to insufficient sequences.\")\n",
    "        continue\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    model = Sequential([\n",
    "        Input(shape=(seq_length, len(features_enhanced))),\n",
    "        Bidirectional(LSTM(128, return_sequences=True)),\n",
    "        Dropout(0.2),\n",
    "        LSTM(64),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    val_split = 0.2 if len(X_train_seq) > 10 else 0.0\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        epochs=100,\n",
    "        batch_size=64,\n",
    "        validation_split=val_split,\n",
    "        callbacks=[early_stop] if val_split > 0 else [],\n",
    "        verbose=2\n",
    "    )\n",
    "\n",
    "    y_pred_lstm = model.predict(X_test_seq)\n",
    "    rmse_lstm = np.sqrt(mean_squared_error(y_test_seq, y_pred_lstm))\n",
    "    mae_lstm = mean_absolute_error(y_test_seq, y_pred_lstm)\n",
    "    mape_lstm = mean_absolute_percentage_error(y_test_seq, y_pred_lstm)\n",
    "    r2_lstm = r2_score(y_test_seq, y_pred_lstm)\n",
    "\n",
    "    results_ma[commodity] = {'RMSE_LSTM': rmse_lstm, 'MAE_LSTM': mae_lstm, 'MAPE_LSTM': mape_lstm, 'R2_LSTM': r2_lstm}\n",
    "    print(f\"Finished processing {commodity}\\n\")"
   ],
   "id": "767c0c45b271aad4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save results\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_ma_df = pd.DataFrame(results_ma).T\n",
    "results_df.to_csv(os.path.join(MODEL_RESULTS_DIR, 'classical_model_results.csv'))\n",
    "results_ma_df.to_csv(os.path.join(MODEL_RESULTS_DIR, 'lstm_ma_results.csv'))\n",
    "\n",
    "print(f\"Saved results to {MODEL_RESULTS_DIR}\")"
   ],
   "id": "f39c1f627570a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
