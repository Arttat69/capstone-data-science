{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-23T15:55:01.937183Z",
     "start_time": "2025-11-23T15:54:56.605981Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, mean_absolute_error, r2_score\n",
    "from sklearn.cluster import KMeans\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import LSTM, Dense, Input, Dropout, Bidirectional, Layer\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.optimizers import Adam\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set paths (same as previous)\n",
    "ROOT = os.getcwd()\n",
    "DATA_DIR = os.path.join(ROOT, \"data\")\n",
    "ENRICHED_DIR = os.path.join(DATA_DIR, \"enriched\")\n",
    "MODEL_RESULTS_DIR = os.path.join(DATA_DIR, 'model_results')\n",
    "os.makedirs(MODEL_RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Commodity names\n",
    "commodities = [\"Gold\", \"WTI\", \"Wheat\", \"NaturalGas\", \"Copper\", \"Lithium\"]\n",
    "\n",
    "# Load enriched data\n",
    "merged_data = {}\n",
    "for name in commodities:\n",
    "    fname = f\"{name.lower()}_enriched.csv\"\n",
    "    path = os.path.join(ENRICHED_DIR, fname)\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path)\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        merged_data[name] = df\n",
    "    else:\n",
    "        print(f\"Missing enriched file for {name}\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T15:55:01.978817Z",
     "start_time": "2025-11-23T15:55:01.960285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def add_lagged_features(df, feature_cols, max_lag=5):\n",
    "    \"\"\"Add lagged features with proper handling\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    for col in feature_cols:\n",
    "        if col in df_copy.columns:\n",
    "            for lag in range(1, max_lag + 1):\n",
    "                df_copy[f'{col}_lag{lag}'] = df_copy[col].shift(lag)\n",
    "    return df_copy\n",
    "\n",
    "def create_sequences(data, feature_cols, target_col, seq_length):\n",
    "    \"\"\"Create sequences for LSTM\"\"\"\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[feature_cols].iloc[i:i+seq_length].values\n",
    "        y = data[target_col].iloc[i+seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "def compute_max_drawdown(returns):\n",
    "    \"\"\"Calculate maximum drawdown from returns series\"\"\"\n",
    "    cumulative = np.cumprod(1 + returns)\n",
    "    running_max = np.maximum.accumulate(cumulative)\n",
    "    drawdown = (cumulative - running_max) / running_max\n",
    "    return np.min(drawdown)\n",
    "\n",
    "def backtest_volatility_strategy(vol_pred, vol_actual, returns, transaction_cost=0.001):\n",
    "    \"\"\"\n",
    "    Trading strategy based on volatility predictions\n",
    "\n",
    "    Strategy Logic:\n",
    "    - When predicted vol > 75th percentile: Reduce position (expect mean reversion)\n",
    "    - When predicted vol < 25th percentile: Increase position (expect trending)\n",
    "    - Use volatility forecast errors to adjust signals\n",
    "    \"\"\"\n",
    "    # Normalize predictions\n",
    "    vol_pred_norm = (vol_pred - np.mean(vol_pred)) / np.std(vol_pred)\n",
    "\n",
    "    # Generate signals based on volatility regime\n",
    "    high_vol_threshold = np.percentile(vol_pred, 75)\n",
    "    low_vol_threshold = np.percentile(vol_pred, 25)\n",
    "\n",
    "    signals = np.zeros(len(vol_pred))\n",
    "    signals[vol_pred > high_vol_threshold] = -0.5  # Reduce exposure in high vol\n",
    "    signals[vol_pred < low_vol_threshold] = 1.0    # Full exposure in low vol\n",
    "    signals[(vol_pred >= low_vol_threshold) & (vol_pred <= high_vol_threshold)] = 0.5\n",
    "\n",
    "    # Align returns (shift by 1 to avoid lookahead bias)\n",
    "    aligned_returns = returns[1:len(signals)+1]\n",
    "    signals = signals[:len(aligned_returns)]\n",
    "\n",
    "    # Calculate strategy returns\n",
    "    strategy_returns = signals * aligned_returns\n",
    "\n",
    "    # Apply transaction costs\n",
    "    position_changes = np.abs(np.diff(np.concatenate([[0], signals])))\n",
    "    transaction_costs = position_changes * transaction_cost\n",
    "    strategy_returns = strategy_returns - transaction_costs[:len(strategy_returns)]\n",
    "\n",
    "    # Metrics\n",
    "    sharpe = np.mean(strategy_returns) / (np.std(strategy_returns) + 1e-10) * np.sqrt(252)\n",
    "    cumulative_return = np.prod(1 + strategy_returns) - 1\n",
    "    max_dd = compute_max_drawdown(strategy_returns)\n",
    "    win_rate = np.mean(strategy_returns > 0)\n",
    "\n",
    "    # Buy & Hold comparison\n",
    "    bh_returns = aligned_returns\n",
    "    bh_sharpe = np.mean(bh_returns) / (np.std(bh_returns) + 1e-10) * np.sqrt(252)\n",
    "\n",
    "    return {\n",
    "        'sharpe': sharpe,\n",
    "        'cumulative_return': cumulative_return * 100,\n",
    "        'max_drawdown': max_dd * 100,\n",
    "        'win_rate': win_rate * 100,\n",
    "        'bh_sharpe': bh_sharpe,\n",
    "        'alpha': sharpe - bh_sharpe,\n",
    "        'avg_return_pct': np.mean(strategy_returns) * 100,\n",
    "        'volatility': np.std(strategy_returns) * 100\n",
    "    }\n",
    "\n",
    "def build_robust_lstm(input_shape, use_bidirectional=False):\n",
    "    \"\"\"\n",
    "    Build LSTM with regularization to prevent overfitting\n",
    "\n",
    "    Key changes from original:\n",
    "    - Smaller network (less prone to memorization)\n",
    "    - L1+L2 regularization\n",
    "    - Huber loss (robust to outliers)\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        LSTM(32, return_sequences=True if use_bidirectional else False,\n",
    "             kernel_regularizer=l1_l2(l1=0.0001, l2=0.001)),\n",
    "        Dropout(0.2),\n",
    "    ])\n",
    "\n",
    "    if use_bidirectional:\n",
    "        model.add(LSTM(16, kernel_regularizer=l1_l2(l1=0.0001, l2=0.001)))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Huber loss is less sensitive to outliers than MSE\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='huber',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def prepare_gprd_features(df, gprd_col='GPRD', method='percentile'):\n",
    "    \"\"\"\n",
    "    Smart GPRD feature engineering to avoid noise\n",
    "\n",
    "    Issue: Raw GPRD may be too noisy for LSTM\n",
    "    Solution: Use regime indicators instead of raw values\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    if method == 'percentile':\n",
    "        # Convert to percentile ranks (0-100)\n",
    "        df_copy['GPRD_percentile'] = df_copy[gprd_col].rank(pct=True) * 100\n",
    "\n",
    "        # Binary high-risk regime\n",
    "        df_copy['GPRD_high_risk'] = (df_copy['GPRD_percentile'] > 75).astype(int)\n",
    "\n",
    "        # Changes in GPRD (captures shocks)\n",
    "        df_copy['GPRD_change'] = df_copy[gprd_col].diff()\n",
    "        df_copy['GPRD_change_ma5'] = df_copy['GPRD_change'].rolling(5).mean()\n",
    "\n",
    "    elif method == 'zscore':\n",
    "        # Z-score normalization\n",
    "        gprd_mean = df_copy[gprd_col].rolling(60).mean()\n",
    "        gprd_std = df_copy[gprd_col].rolling(60).std()\n",
    "        df_copy['GPRD_zscore'] = (df_copy[gprd_col] - gprd_mean) / (gprd_std + 1e-10)\n",
    "        df_copy['GPRD_zscore'] = df_copy['GPRD_zscore'].clip(-3, 3)  # Remove extreme outliers\n",
    "\n",
    "    return df_copy\n"
   ],
   "id": "64772a4d221d3a53",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T15:55:02.404400Z",
     "start_time": "2025-11-23T15:55:02.361569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# MAIN MODELING PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def run_enhanced_lstm_with_gprd(commodity_name, df, use_gprd=True):\n",
    "    \"\"\"\n",
    "    Run LSTM with optional GPRD integration and full backtesting\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Processing: {commodity_name} (GPRD: {'ON' if use_gprd else 'OFF'})\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # Prepare data\n",
    "    df = df.copy()\n",
    "    df = df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "    # CRITICAL FIX: Detect and rename commodity-specific columns\n",
    "    close_col = None\n",
    "    for col in df.columns:\n",
    "        if 'Close' in col or 'close' in col:\n",
    "            close_col = col\n",
    "            break\n",
    "\n",
    "    if close_col is None:\n",
    "        print(f\"ERROR: No Close column found in {df.columns.tolist()}\")\n",
    "        return None\n",
    "\n",
    "    # Rename to standard name if needed\n",
    "    if close_col != 'Close':\n",
    "        df['Close'] = df[close_col]\n",
    "        print(f\"Renamed {close_col} -> Close\")\n",
    "\n",
    "    # Calculate Return and Vol_5 if missing\n",
    "    if 'Return' not in df.columns:\n",
    "        df['Return'] = df['Close'].pct_change()\n",
    "        print(\"Calculated Return from Close prices\")\n",
    "\n",
    "    if 'Vol_5' not in df.columns:\n",
    "        df['Vol_5'] = df['Return'].rolling(5).std()\n",
    "        print(\"Calculated Vol_5 from Returns\")\n",
    "\n",
    "    # Identify price column (could be 'Close', 'Price', or 'PRICE')\n",
    "    price_col = 'Close'  # We just ensured this exists above\n",
    "\n",
    "    # Add technical indicators if not present\n",
    "    if 'MA_5' not in df.columns:\n",
    "        df['MA_5'] = df['Close'].rolling(5).mean()\n",
    "    if 'MA_20' not in df.columns:\n",
    "        df['MA_20'] = df['Close'].rolling(20).mean()\n",
    "\n",
    "    # Feature engineering for GPRD\n",
    "    if use_gprd and 'GPRD' in df.columns:\n",
    "        df = prepare_gprd_features(df, method='percentile')\n",
    "        gprd_features = ['GPRD_percentile', 'GPRD_high_risk', 'GPRD_change_ma5']\n",
    "    else:\n",
    "        gprd_features = []\n",
    "\n",
    "    # Base features (always included)\n",
    "    base_features = ['Return', 'Vol_5', 'MA_5', 'MA_20']\n",
    "\n",
    "     # Add MA features only if they were successfully created\n",
    "    if 'MA_5' in df.columns and not df['MA_5'].isna().all():\n",
    "        base_features.append('MA_5')\n",
    "    if 'MA_20' in df.columns and not df['MA_20'].isna().all():\n",
    "        base_features.append('MA_20')\n",
    "\n",
    "    # Add sentiment if available\n",
    "    if 'sentiment' in df.columns:\n",
    "        df['sentiment'].fillna(0, inplace=True)\n",
    "        base_features.append('sentiment')\n",
    "\n",
    "    if 'geo_keyword_hits' in df.columns:\n",
    "        df['geo_keyword_hits'].fillna(0, inplace=True)\n",
    "        base_features.append('geo_keyword_hits')\n",
    "\n",
    "    # Create lagged features\n",
    "    features_to_lag = base_features + gprd_features\n",
    "    df = add_lagged_features(df, features_to_lag, max_lag=5)\n",
    "\n",
    "    # Construct feature list - ONLY use features that actually exist\n",
    "    feature_cols = []\n",
    "    for feat in features_to_lag:\n",
    "        for lag in range(1, 6):\n",
    "            lag_col = f'{feat}_lag{lag}'\n",
    "            if lag_col in df.columns:\n",
    "                feature_cols.append(lag_col)\n",
    "\n",
    "    print(f\"Features ({len(feature_cols)}): {feature_cols[:3]}... (showing first 3)\")\n",
    "\n",
    "     # Remove features with too many NaNs - check they all exist first\n",
    "    cols_to_keep = ['Date', 'Vol_5', 'Return']\n",
    "    for col in feature_cols:\n",
    "        if col in df.columns:\n",
    "            cols_to_keep.append(col)\n",
    "\n",
    "    df_clean = df[cols_to_keep].copy()\n",
    "    df_clean = df_clean.dropna()\n",
    "\n",
    "     # Update feature_cols to only include columns that survived\n",
    "    feature_cols = [col for col in feature_cols if col in df_clean.columns]\n",
    "\n",
    "    print(f\"Clean data shape: {df_clean.shape}\")\n",
    "    print(f\"Final feature count: {len(feature_cols)}\")\n",
    "\n",
    "    if len(df_clean) < 100:\n",
    "        print(f\"Insufficient data after cleaning. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    # Train/test split\n",
    "    split_date = pd.to_datetime('2018-01-01')\n",
    "    train_df = df_clean[pd.to_datetime(df_clean['Date']) < split_date].copy()\n",
    "    test_df = df_clean[pd.to_datetime(df_clean['Date']) >= split_date].copy()\n",
    "\n",
    "    print(f\"Train: {len(train_df)} samples | Test: {len(test_df)} samples\")\n",
    "\n",
    "    if len(train_df) < 50 or len(test_df) < 20:\n",
    "        print(\"Insufficient train/test data. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    # Target and features\n",
    "    target = 'Vol_5'\n",
    "\n",
    "    # Scaling with RobustScaler (less sensitive to outliers)\n",
    "    scaler = RobustScaler()\n",
    "\n",
    "    # ============= NEW: STRICT FEATURE VALIDATION =============\n",
    "    # Only use features that exist in BOTH train and test\n",
    "    available_features = [col for col in feature_cols\n",
    "                         if col in train_df.columns and col in test_df.columns]\n",
    "\n",
    "    if len(available_features) == 0:\n",
    "        print(\"ERROR: No valid features found after train/test split\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Using {len(available_features)} features for training\")\n",
    "\n",
    "     # ============= NEW: DEBUG - CHECK ACTUAL COLUMNS =============\n",
    "    print(f\"DEBUG: train_df has {len(train_df.columns)} columns\")\n",
    "    print(f\"DEBUG: Requesting {len(available_features)} features\")\n",
    "\n",
    "    # # ============= NEW: RESET INDEX BEFORE SCALING =============\n",
    "    # train_subset = train_df[available_features].reset_index(drop=True)\n",
    "    # test_subset = test_df[available_features].reset_index(drop=True)\n",
    "    #\n",
    "    # train_features_scaled = scaler.fit_transform(train_subset)\n",
    "    # test_features_scaled = scaler.transform(test_subset)\n",
    "\n",
    "     # ============= NEW: EXPLICIT COLUMN SELECTION =============\n",
    "    train_subset = train_df[available_features].copy()\n",
    "    test_subset = test_df[available_features].copy()\n",
    "\n",
    "    print(f\"DEBUG: train_subset shape BEFORE scaling: {train_subset.shape}\")\n",
    "\n",
    "    train_features_scaled = scaler.fit_transform(train_subset)\n",
    "    test_features_scaled = scaler.transform(test_subset)\n",
    "\n",
    "    print(f\"DEBUG: Scaled array shape: {train_features_scaled.shape}\")\n",
    "    print(f\"DEBUG: Number of column names: {len(available_features)}\")\n",
    "\n",
    "    # ============= CREATE DATAFRAMES WITH CORRECT INDEX =============\n",
    "    if train_features_scaled.shape[1] != len(available_features):\n",
    "        print(f\"ERROR: Shape mismatch! Array has {train_features_scaled.shape[1]} cols but {len(available_features)} names\")\n",
    "        print(f\"Available features: {available_features}\")\n",
    "        return None\n",
    "\n",
    "    train_scaled_df = pd.DataFrame(\n",
    "        train_features_scaled,\n",
    "        columns=available_features\n",
    "    )\n",
    "    test_scaled_df = pd.DataFrame(\n",
    "        test_features_scaled,\n",
    "        columns=available_features\n",
    "    )\n",
    "\n",
    "    train_scaled_df[target] = train_df[target].values\n",
    "    test_scaled_df[target] = test_df[target].values\n",
    "    train_scaled_df['Return'] = train_df['Return'].values\n",
    "    test_scaled_df['Return'] = test_df['Return'].values\n",
    "\n",
    "    # Create sequences\n",
    "    seq_length = 20\n",
    "    X_train_seq, y_train_seq = create_sequences(train_scaled_df, feature_cols, target, seq_length)\n",
    "    X_test_seq, y_test_seq = create_sequences(test_scaled_df, feature_cols, target, seq_length)\n",
    "\n",
    "    print(f\"Sequences - Train: {X_train_seq.shape} | Test: {X_test_seq.shape}\")\n",
    "\n",
    "    if len(X_train_seq) < 20 or len(X_test_seq) < 5:\n",
    "        print(\"Insufficient sequences. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    # Build and train model\n",
    "    model = build_robust_lstm(input_shape=(seq_length, len(feature_cols)))\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=0)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001, verbose=0)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        epochs=100,\n",
    "        batch_size=64,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_train = model.predict(X_train_seq, verbose=0).squeeze()\n",
    "    y_pred_test = model.predict(X_test_seq, verbose=0).squeeze()\n",
    "\n",
    "    # Evaluation metrics\n",
    "    train_r2 = r2_score(y_train_seq, y_pred_train)\n",
    "    test_r2 = r2_score(y_test_seq, y_pred_test)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test_seq, y_pred_test))\n",
    "    test_mae = mean_absolute_error(y_test_seq, y_pred_test)\n",
    "\n",
    "    print(f\"\\nModel Performance:\")\n",
    "    print(f\"  Train R²: {train_r2:.4f} | Test R²: {test_r2:.4f}\")\n",
    "    print(f\"  Test RMSE: {test_rmse:.4f} | Test MAE: {test_mae:.4f}\")\n",
    "\n",
    "    # Extract returns for backtesting\n",
    "    test_returns = test_scaled_df['Return'].iloc[seq_length:].values\n",
    "\n",
    "    # Backtest trading strategy\n",
    "    backtest_results = backtest_volatility_strategy(\n",
    "        y_pred_test, y_test_seq, test_returns, transaction_cost=0.001\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTrading Strategy Performance:\")\n",
    "    print(f\"  Sharpe Ratio: {backtest_results['sharpe']:.3f}\")\n",
    "    print(f\"  Alpha (vs B&H): {backtest_results['alpha']:.3f}\")\n",
    "    print(f\"  Cumulative Return: {backtest_results['cumulative_return']:.2f}%\")\n",
    "    print(f\"  Max Drawdown: {backtest_results['max_drawdown']:.2f}%\")\n",
    "    print(f\"  Win Rate: {backtest_results['win_rate']:.2f}%\")\n",
    "\n",
    "    # Compile results\n",
    "    results = {\n",
    "        'commodity': commodity_name,\n",
    "        'use_gprd': use_gprd,\n",
    "        'n_features': len(feature_cols),\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mae': test_mae,\n",
    "        **backtest_results\n",
    "    }\n",
    "    # Visualization\n",
    "    plot_results(commodity_name, y_test_seq, y_pred_test, test_returns, history, use_gprd)\n",
    "\n",
    "    return results\n",
    "\n",
    "def plot_results(commodity, y_true, y_pred, returns, history, use_gprd):\n",
    "    \"\"\"Create comprehensive visualization\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(f'{commodity} - LSTM Results (GPRD: {\"ON\" if use_gprd else \"OFF\"})', fontsize=16)\n",
    "\n",
    "    # Plot 1: Predictions vs Actual\n",
    "    axes[0, 0].plot(y_true[-200:], label='Actual Vol', alpha=0.7, linewidth=2)\n",
    "    axes[0, 0].plot(y_pred[-200:], label='Predicted Vol', alpha=0.7, linewidth=2)\n",
    "    axes[0, 0].set_title('Volatility Forecast (Last 200 days)')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "    # Plot 2: Training history\n",
    "    axes[0, 1].plot(history.history['loss'], label='Train Loss')\n",
    "    axes[0, 1].plot(history.history['val_loss'], label='Val Loss')\n",
    "    axes[0, 1].set_title('Training History')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "    # Plot 3: Residuals\n",
    "    residuals = y_true - y_pred\n",
    "    axes[1, 0].scatter(y_pred, residuals, alpha=0.3)\n",
    "    axes[1, 0].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[1, 0].set_title('Residual Plot')\n",
    "    axes[1, 0].set_xlabel('Predicted')\n",
    "    axes[1, 0].set_ylabel('Residuals')\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "    # Plot 4: Distribution comparison\n",
    "    axes[1, 1].hist(y_true, bins=30, alpha=0.5, label='Actual', density=True)\n",
    "    axes[1, 1].hist(y_pred, bins=30, alpha=0.5, label='Predicted', density=True)\n",
    "    axes[1, 1].set_title('Distribution Comparison')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(MODEL_RESULTS_DIR, f'{commodity}_lstm_{\"gprd\" if use_gprd else \"nogprd\"}.png'), dpi=150)\n",
    "    plt.show()\n"
   ],
   "id": "2091515121ac2177",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T15:55:05.931232Z",
     "start_time": "2025-11-23T15:55:02.467253Z"
    }
   },
   "cell_type": "code",
   "source": [
    " # ============================================================================\n",
    "# RUN EXPERIMENTS\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    print(\"Loading enriched data...\")\n",
    "    merged_data = {}\n",
    "    for name in commodities:\n",
    "        fname = f\"{name.lower()}_enriched.csv\"\n",
    "        path = os.path.join(ENRICHED_DIR, fname)\n",
    "        if os.path.exists(path):\n",
    "            df = pd.read_csv(path)\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "            merged_data[name] = df\n",
    "            print(f\"  Loaded {name}: {len(df)} rows, Columns: {df.columns.tolist()[:5]}...\")\n",
    "        else:\n",
    "            print(f\"  Missing: {name}\")\n",
    "\n",
    "    # Run experiments: WITH and WITHOUT GPRD\n",
    "    all_results = []\n",
    "\n",
    "    for commodity in merged_data.keys():\n",
    "        # Without GPRD\n",
    "        results_no_gprd = run_enhanced_lstm_with_gprd(commodity, merged_data[commodity], use_gprd=False)\n",
    "        if results_no_gprd:\n",
    "            all_results.append(results_no_gprd)\n",
    "\n",
    "        # With GPRD (if available)\n",
    "        if 'GPRD' in merged_data[commodity].columns:\n",
    "            results_with_gprd = run_enhanced_lstm_with_gprd(commodity, merged_data[commodity], use_gprd=True)\n",
    "            if results_with_gprd:\n",
    "                all_results.append(results_with_gprd)\n",
    "\n",
    "    # Save results\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_df.to_csv(os.path.join(MODEL_RESULTS_DIR, 'lstm_comparison_gprd.csv'), index=False)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY: GPRD Impact Analysis\")\n",
    "    print(\"=\"*80)\n",
    "    print(results_df[['commodity', 'use_gprd', 'test_r2', 'sharpe', 'alpha', 'cumulative_return']])\n",
    "\n",
    "    # Statistical comparison\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Average Performance by GPRD Usage:\")\n",
    "    print(\"=\"*80)\n",
    "    print(results_df.groupby('use_gprd')[['test_r2', 'sharpe', 'alpha']].mean())"
   ],
   "id": "d752485d2b5a685c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading enriched data...\n",
      "  Loaded Gold: 5342 rows, Columns: ['Date', 'Close_GC=F', 'High_GC=F', 'Low_GC=F', 'Open_GC=F']...\n",
      "  Loaded WTI: 5351 rows, Columns: ['Date', 'Close_CL=F', 'High_CL=F', 'Low_CL=F', 'Open_CL=F']...\n",
      "  Loaded Wheat: 5367 rows, Columns: ['Date', 'Close_ZW=F', 'High_ZW=F', 'Low_ZW=F', 'Open_ZW=F']...\n",
      "  Loaded NaturalGas: 3694 rows, Columns: ['Date', 'Close_UNG', 'High_UNG', 'Low_UNG', 'Open_UNG']...\n",
      "  Loaded Copper: 5346 rows, Columns: ['Date', 'Close_HG=F', 'High_HG=F', 'Low_HG=F', 'Open_HG=F']...\n",
      "  Loaded Lithium: 2871 rows, Columns: ['Date', 'Close_LIT', 'High_LIT', 'Low_LIT', 'Open_LIT']...\n",
      "\n",
      "================================================================================\n",
      "Processing: Gold (GPRD: OFF)\n",
      "================================================================================\n",
      "Renamed Close_GC=F -> Close\n",
      "Features (40): ['Return_lag1', 'Return_lag2', 'Return_lag3']... (showing first 3)\n",
      "Clean data shape: (5318, 43)\n",
      "Final feature count: 40\n",
      "Train: 4311 samples | Test: 1007 samples\n",
      "Using 40 features for training\n",
      "DEBUG: train_df has 43 columns\n",
      "DEBUG: Requesting 40 features\n",
      "DEBUG: train_subset shape BEFORE scaling: (4311, 60)\n",
      "DEBUG: Scaled array shape: (4311, 60)\n",
      "DEBUG: Number of column names: 40\n",
      "ERROR: Shape mismatch! Array has 60 cols but 40 names\n",
      "Available features: ['Return_lag1', 'Return_lag2', 'Return_lag3', 'Return_lag4', 'Return_lag5', 'Vol_5_lag1', 'Vol_5_lag2', 'Vol_5_lag3', 'Vol_5_lag4', 'Vol_5_lag5', 'MA_5_lag1', 'MA_5_lag2', 'MA_5_lag3', 'MA_5_lag4', 'MA_5_lag5', 'MA_20_lag1', 'MA_20_lag2', 'MA_20_lag3', 'MA_20_lag4', 'MA_20_lag5', 'MA_5_lag1', 'MA_5_lag2', 'MA_5_lag3', 'MA_5_lag4', 'MA_5_lag5', 'MA_20_lag1', 'MA_20_lag2', 'MA_20_lag3', 'MA_20_lag4', 'MA_20_lag5', 'sentiment_lag1', 'sentiment_lag2', 'sentiment_lag3', 'sentiment_lag4', 'sentiment_lag5', 'geo_keyword_hits_lag1', 'geo_keyword_hits_lag2', 'geo_keyword_hits_lag3', 'geo_keyword_hits_lag4', 'geo_keyword_hits_lag5']\n",
      "\n",
      "================================================================================\n",
      "Processing: Gold (GPRD: ON)\n",
      "================================================================================\n",
      "Renamed Close_GC=F -> Close\n",
      "Features (55): ['Return_lag1', 'Return_lag2', 'Return_lag3']... (showing first 3)\n",
      "Clean data shape: (5318, 58)\n",
      "Final feature count: 55\n",
      "Train: 4311 samples | Test: 1007 samples\n",
      "Using 55 features for training\n",
      "DEBUG: train_df has 58 columns\n",
      "DEBUG: Requesting 55 features\n",
      "DEBUG: train_subset shape BEFORE scaling: (4311, 75)\n",
      "DEBUG: Scaled array shape: (4311, 75)\n",
      "DEBUG: Number of column names: 55\n",
      "ERROR: Shape mismatch! Array has 75 cols but 55 names\n",
      "Available features: ['Return_lag1', 'Return_lag2', 'Return_lag3', 'Return_lag4', 'Return_lag5', 'Vol_5_lag1', 'Vol_5_lag2', 'Vol_5_lag3', 'Vol_5_lag4', 'Vol_5_lag5', 'MA_5_lag1', 'MA_5_lag2', 'MA_5_lag3', 'MA_5_lag4', 'MA_5_lag5', 'MA_20_lag1', 'MA_20_lag2', 'MA_20_lag3', 'MA_20_lag4', 'MA_20_lag5', 'MA_5_lag1', 'MA_5_lag2', 'MA_5_lag3', 'MA_5_lag4', 'MA_5_lag5', 'MA_20_lag1', 'MA_20_lag2', 'MA_20_lag3', 'MA_20_lag4', 'MA_20_lag5', 'sentiment_lag1', 'sentiment_lag2', 'sentiment_lag3', 'sentiment_lag4', 'sentiment_lag5', 'geo_keyword_hits_lag1', 'geo_keyword_hits_lag2', 'geo_keyword_hits_lag3', 'geo_keyword_hits_lag4', 'geo_keyword_hits_lag5', 'GPRD_percentile_lag1', 'GPRD_percentile_lag2', 'GPRD_percentile_lag3', 'GPRD_percentile_lag4', 'GPRD_percentile_lag5', 'GPRD_high_risk_lag1', 'GPRD_high_risk_lag2', 'GPRD_high_risk_lag3', 'GPRD_high_risk_lag4', 'GPRD_high_risk_lag5', 'GPRD_change_ma5_lag1', 'GPRD_change_ma5_lag2', 'GPRD_change_ma5_lag3', 'GPRD_change_ma5_lag4', 'GPRD_change_ma5_lag5']\n",
      "\n",
      "================================================================================\n",
      "Processing: WTI (GPRD: OFF)\n",
      "================================================================================\n",
      "Renamed Close_CL=F -> Close\n",
      "Features (40): ['Return_lag1', 'Return_lag2', 'Return_lag3']... (showing first 3)\n",
      "Clean data shape: (5327, 43)\n",
      "Final feature count: 40\n",
      "Train: 4319 samples | Test: 1008 samples\n",
      "Using 40 features for training\n",
      "DEBUG: train_df has 43 columns\n",
      "DEBUG: Requesting 40 features\n",
      "DEBUG: train_subset shape BEFORE scaling: (4319, 60)\n",
      "DEBUG: Scaled array shape: (4319, 60)\n",
      "DEBUG: Number of column names: 40\n",
      "ERROR: Shape mismatch! Array has 60 cols but 40 names\n",
      "Available features: ['Return_lag1', 'Return_lag2', 'Return_lag3', 'Return_lag4', 'Return_lag5', 'Vol_5_lag1', 'Vol_5_lag2', 'Vol_5_lag3', 'Vol_5_lag4', 'Vol_5_lag5', 'MA_5_lag1', 'MA_5_lag2', 'MA_5_lag3', 'MA_5_lag4', 'MA_5_lag5', 'MA_20_lag1', 'MA_20_lag2', 'MA_20_lag3', 'MA_20_lag4', 'MA_20_lag5', 'MA_5_lag1', 'MA_5_lag2', 'MA_5_lag3', 'MA_5_lag4', 'MA_5_lag5', 'MA_20_lag1', 'MA_20_lag2', 'MA_20_lag3', 'MA_20_lag4', 'MA_20_lag5', 'sentiment_lag1', 'sentiment_lag2', 'sentiment_lag3', 'sentiment_lag4', 'sentiment_lag5', 'geo_keyword_hits_lag1', 'geo_keyword_hits_lag2', 'geo_keyword_hits_lag3', 'geo_keyword_hits_lag4', 'geo_keyword_hits_lag5']\n",
      "\n",
      "================================================================================\n",
      "Processing: WTI (GPRD: ON)\n",
      "================================================================================\n",
      "Renamed Close_CL=F -> Close\n",
      "Features (55): ['Return_lag1', 'Return_lag2', 'Return_lag3']... (showing first 3)\n",
      "Clean data shape: (5327, 58)\n",
      "Final feature count: 55\n",
      "Train: 4319 samples | Test: 1008 samples\n",
      "Using 55 features for training\n",
      "DEBUG: train_df has 58 columns\n",
      "DEBUG: Requesting 55 features\n",
      "DEBUG: train_subset shape BEFORE scaling: (4319, 75)\n",
      "DEBUG: Scaled array shape: (4319, 75)\n",
      "DEBUG: Number of column names: 55\n",
      "ERROR: Shape mismatch! Array has 75 cols but 55 names\n",
      "Available features: ['Return_lag1', 'Return_lag2', 'Return_lag3', 'Return_lag4', 'Return_lag5', 'Vol_5_lag1', 'Vol_5_lag2', 'Vol_5_lag3', 'Vol_5_lag4', 'Vol_5_lag5', 'MA_5_lag1', 'MA_5_lag2', 'MA_5_lag3', 'MA_5_lag4', 'MA_5_lag5', 'MA_20_lag1', 'MA_20_lag2', 'MA_20_lag3', 'MA_20_lag4', 'MA_20_lag5', 'MA_5_lag1', 'MA_5_lag2', 'MA_5_lag3', 'MA_5_lag4', 'MA_5_lag5', 'MA_20_lag1', 'MA_20_lag2', 'MA_20_lag3', 'MA_20_lag4', 'MA_20_lag5', 'sentiment_lag1', 'sentiment_lag2', 'sentiment_lag3', 'sentiment_lag4', 'sentiment_lag5', 'geo_keyword_hits_lag1', 'geo_keyword_hits_lag2', 'geo_keyword_hits_lag3', 'geo_keyword_hits_lag4', 'geo_keyword_hits_lag5', 'GPRD_percentile_lag1', 'GPRD_percentile_lag2', 'GPRD_percentile_lag3', 'GPRD_percentile_lag4', 'GPRD_percentile_lag5', 'GPRD_high_risk_lag1', 'GPRD_high_risk_lag2', 'GPRD_high_risk_lag3', 'GPRD_high_risk_lag4', 'GPRD_high_risk_lag5', 'GPRD_change_ma5_lag1', 'GPRD_change_ma5_lag2', 'GPRD_change_ma5_lag3', 'GPRD_change_ma5_lag4', 'GPRD_change_ma5_lag5']\n",
      "\n",
      "================================================================================\n",
      "Processing: Wheat (GPRD: OFF)\n",
      "================================================================================\n",
      "Renamed Close_ZW=F -> Close\n",
      "Features (40): ['Return_lag1', 'Return_lag2', 'Return_lag3']... (showing first 3)\n",
      "Clean data shape: (5343, 43)\n",
      "Final feature count: 40\n",
      "Train: 4335 samples | Test: 1008 samples\n",
      "Using 40 features for training\n",
      "DEBUG: train_df has 43 columns\n",
      "DEBUG: Requesting 40 features\n",
      "DEBUG: train_subset shape BEFORE scaling: (4335, 60)\n",
      "DEBUG: Scaled array shape: (4335, 60)\n",
      "DEBUG: Number of column names: 40\n",
      "ERROR: Shape mismatch! Array has 60 cols but 40 names\n",
      "Available features: ['Return_lag1', 'Return_lag2', 'Return_lag3', 'Return_lag4', 'Return_lag5', 'Vol_5_lag1', 'Vol_5_lag2', 'Vol_5_lag3', 'Vol_5_lag4', 'Vol_5_lag5', 'MA_5_lag1', 'MA_5_lag2', 'MA_5_lag3', 'MA_5_lag4', 'MA_5_lag5', 'MA_20_lag1', 'MA_20_lag2', 'MA_20_lag3', 'MA_20_lag4', 'MA_20_lag5', 'MA_5_lag1', 'MA_5_lag2', 'MA_5_lag3', 'MA_5_lag4', 'MA_5_lag5', 'MA_20_lag1', 'MA_20_lag2', 'MA_20_lag3', 'MA_20_lag4', 'MA_20_lag5', 'sentiment_lag1', 'sentiment_lag2', 'sentiment_lag3', 'sentiment_lag4', 'sentiment_lag5', 'geo_keyword_hits_lag1', 'geo_keyword_hits_lag2', 'geo_keyword_hits_lag3', 'geo_keyword_hits_lag4', 'geo_keyword_hits_lag5']\n",
      "\n",
      "================================================================================\n",
      "Processing: Wheat (GPRD: ON)\n",
      "================================================================================\n",
      "Renamed Close_ZW=F -> Close\n",
      "Features (55): ['Return_lag1', 'Return_lag2', 'Return_lag3']... (showing first 3)\n",
      "Clean data shape: (5343, 58)\n",
      "Final feature count: 55\n",
      "Train: 4335 samples | Test: 1008 samples\n",
      "Using 55 features for training\n",
      "DEBUG: train_df has 58 columns\n",
      "DEBUG: Requesting 55 features\n",
      "DEBUG: train_subset shape BEFORE scaling: (4335, 75)\n",
      "DEBUG: Scaled array shape: (4335, 75)\n",
      "DEBUG: Number of column names: 55\n",
      "ERROR: Shape mismatch! Array has 75 cols but 55 names\n",
      "Available features: ['Return_lag1', 'Return_lag2', 'Return_lag3', 'Return_lag4', 'Return_lag5', 'Vol_5_lag1', 'Vol_5_lag2', 'Vol_5_lag3', 'Vol_5_lag4', 'Vol_5_lag5', 'MA_5_lag1', 'MA_5_lag2', 'MA_5_lag3', 'MA_5_lag4', 'MA_5_lag5', 'MA_20_lag1', 'MA_20_lag2', 'MA_20_lag3', 'MA_20_lag4', 'MA_20_lag5', 'MA_5_lag1', 'MA_5_lag2', 'MA_5_lag3', 'MA_5_lag4', 'MA_5_lag5', 'MA_20_lag1', 'MA_20_lag2', 'MA_20_lag3', 'MA_20_lag4', 'MA_20_lag5', 'sentiment_lag1', 'sentiment_lag2', 'sentiment_lag3', 'sentiment_lag4', 'sentiment_lag5', 'geo_keyword_hits_lag1', 'geo_keyword_hits_lag2', 'geo_keyword_hits_lag3', 'geo_keyword_hits_lag4', 'geo_keyword_hits_lag5', 'GPRD_percentile_lag1', 'GPRD_percentile_lag2', 'GPRD_percentile_lag3', 'GPRD_percentile_lag4', 'GPRD_percentile_lag5', 'GPRD_high_risk_lag1', 'GPRD_high_risk_lag2', 'GPRD_high_risk_lag3', 'GPRD_high_risk_lag4', 'GPRD_high_risk_lag5', 'GPRD_change_ma5_lag1', 'GPRD_change_ma5_lag2', 'GPRD_change_ma5_lag3', 'GPRD_change_ma5_lag4', 'GPRD_change_ma5_lag5']\n",
      "\n",
      "================================================================================\n",
      "Processing: NaturalGas (GPRD: OFF)\n",
      "================================================================================\n",
      "Renamed Close_UNG -> Close\n",
      "Features (40): ['Return_lag1', 'Return_lag2', 'Return_lag3']... (showing first 3)\n",
      "Clean data shape: (3670, 43)\n",
      "Final feature count: 40\n",
      "Train: 2662 samples | Test: 1008 samples\n",
      "Using 40 features for training\n",
      "DEBUG: train_df has 43 columns\n",
      "DEBUG: Requesting 40 features\n",
      "DEBUG: train_subset shape BEFORE scaling: (2662, 60)\n",
      "DEBUG: Scaled array shape: (2662, 60)\n",
      "DEBUG: Number of column names: 40\n",
      "ERROR: Shape mismatch! Array has 60 cols but 40 names\n",
      "Available features: ['Return_lag1', 'Return_lag2', 'Return_lag3', 'Return_lag4', 'Return_lag5', 'Vol_5_lag1', 'Vol_5_lag2', 'Vol_5_lag3', 'Vol_5_lag4', 'Vol_5_lag5', 'MA_5_lag1', 'MA_5_lag2', 'MA_5_lag3', 'MA_5_lag4', 'MA_5_lag5', 'MA_20_lag1', 'MA_20_lag2', 'MA_20_lag3', 'MA_20_lag4', 'MA_20_lag5', 'MA_5_lag1', 'MA_5_lag2', 'MA_5_lag3', 'MA_5_lag4', 'MA_5_lag5', 'MA_20_lag1', 'MA_20_lag2', 'MA_20_lag3', 'MA_20_lag4', 'MA_20_lag5', 'sentiment_lag1', 'sentiment_lag2', 'sentiment_lag3', 'sentiment_lag4', 'sentiment_lag5', 'geo_keyword_hits_lag1', 'geo_keyword_hits_lag2', 'geo_keyword_hits_lag3', 'geo_keyword_hits_lag4', 'geo_keyword_hits_lag5']\n",
      "\n",
      "================================================================================\n",
      "Processing: NaturalGas (GPRD: ON)\n",
      "================================================================================\n",
      "Renamed Close_UNG -> Close\n",
      "Features (55): ['Return_lag1', 'Return_lag2', 'Return_lag3']... (showing first 3)\n",
      "Clean data shape: (3670, 58)\n",
      "Final feature count: 55\n",
      "Train: 2662 samples | Test: 1008 samples\n",
      "Using 55 features for training\n",
      "DEBUG: train_df has 58 columns\n",
      "DEBUG: Requesting 55 features\n",
      "DEBUG: train_subset shape BEFORE scaling: (2662, 75)\n",
      "DEBUG: Scaled array shape: (2662, 75)\n",
      "DEBUG: Number of column names: 55\n",
      "ERROR: Shape mismatch! Array has 75 cols but 55 names\n",
      "Available features: ['Return_lag1', 'Return_lag2', 'Return_lag3', 'Return_lag4', 'Return_lag5', 'Vol_5_lag1', 'Vol_5_lag2', 'Vol_5_lag3', 'Vol_5_lag4', 'Vol_5_lag5', 'MA_5_lag1', 'MA_5_lag2', 'MA_5_lag3', 'MA_5_lag4', 'MA_5_lag5', 'MA_20_lag1', 'MA_20_lag2', 'MA_20_lag3', 'MA_20_lag4', 'MA_20_lag5', 'MA_5_lag1', 'MA_5_lag2', 'MA_5_lag3', 'MA_5_lag4', 'MA_5_lag5', 'MA_20_lag1', 'MA_20_lag2', 'MA_20_lag3', 'MA_20_lag4', 'MA_20_lag5', 'sentiment_lag1', 'sentiment_lag2', 'sentiment_lag3', 'sentiment_lag4', 'sentiment_lag5', 'geo_keyword_hits_lag1', 'geo_keyword_hits_lag2', 'geo_keyword_hits_lag3', 'geo_keyword_hits_lag4', 'geo_keyword_hits_lag5', 'GPRD_percentile_lag1', 'GPRD_percentile_lag2', 'GPRD_percentile_lag3', 'GPRD_percentile_lag4', 'GPRD_percentile_lag5', 'GPRD_high_risk_lag1', 'GPRD_high_risk_lag2', 'GPRD_high_risk_lag3', 'GPRD_high_risk_lag4', 'GPRD_high_risk_lag5', 'GPRD_change_ma5_lag1', 'GPRD_change_ma5_lag2', 'GPRD_change_ma5_lag3', 'GPRD_change_ma5_lag4', 'GPRD_change_ma5_lag5']\n",
      "\n",
      "================================================================================\n",
      "Processing: Copper (GPRD: OFF)\n",
      "================================================================================\n",
      "Renamed Close_HG=F -> Close\n",
      "Features (40): ['Return_lag1', 'Return_lag2', 'Return_lag3']... (showing first 3)\n",
      "Clean data shape: (5322, 43)\n",
      "Final feature count: 40\n",
      "Train: 4315 samples | Test: 1007 samples\n",
      "Using 40 features for training\n",
      "DEBUG: train_df has 43 columns\n",
      "DEBUG: Requesting 40 features\n",
      "DEBUG: train_subset shape BEFORE scaling: (4315, 60)\n",
      "DEBUG: Scaled array shape: (4315, 60)\n",
      "DEBUG: Number of column names: 40\n",
      "ERROR: Shape mismatch! Array has 60 cols but 40 names\n",
      "Available features: ['Return_lag1', 'Return_lag2', 'Return_lag3', 'Return_lag4', 'Return_lag5', 'Vol_5_lag1', 'Vol_5_lag2', 'Vol_5_lag3', 'Vol_5_lag4', 'Vol_5_lag5', 'MA_5_lag1', 'MA_5_lag2', 'MA_5_lag3', 'MA_5_lag4', 'MA_5_lag5', 'MA_20_lag1', 'MA_20_lag2', 'MA_20_lag3', 'MA_20_lag4', 'MA_20_lag5', 'MA_5_lag1', 'MA_5_lag2', 'MA_5_lag3', 'MA_5_lag4', 'MA_5_lag5', 'MA_20_lag1', 'MA_20_lag2', 'MA_20_lag3', 'MA_20_lag4', 'MA_20_lag5', 'sentiment_lag1', 'sentiment_lag2', 'sentiment_lag3', 'sentiment_lag4', 'sentiment_lag5', 'geo_keyword_hits_lag1', 'geo_keyword_hits_lag2', 'geo_keyword_hits_lag3', 'geo_keyword_hits_lag4', 'geo_keyword_hits_lag5']\n",
      "\n",
      "================================================================================\n",
      "Processing: Copper (GPRD: ON)\n",
      "================================================================================\n",
      "Renamed Close_HG=F -> Close\n",
      "Features (55): ['Return_lag1', 'Return_lag2', 'Return_lag3']... (showing first 3)\n",
      "Clean data shape: (5322, 58)\n",
      "Final feature count: 55\n",
      "Train: 4315 samples | Test: 1007 samples\n",
      "Using 55 features for training\n",
      "DEBUG: train_df has 58 columns\n",
      "DEBUG: Requesting 55 features\n",
      "DEBUG: train_subset shape BEFORE scaling: (4315, 75)\n",
      "DEBUG: Scaled array shape: (4315, 75)\n",
      "DEBUG: Number of column names: 55\n",
      "ERROR: Shape mismatch! Array has 75 cols but 55 names\n",
      "Available features: ['Return_lag1', 'Return_lag2', 'Return_lag3', 'Return_lag4', 'Return_lag5', 'Vol_5_lag1', 'Vol_5_lag2', 'Vol_5_lag3', 'Vol_5_lag4', 'Vol_5_lag5', 'MA_5_lag1', 'MA_5_lag2', 'MA_5_lag3', 'MA_5_lag4', 'MA_5_lag5', 'MA_20_lag1', 'MA_20_lag2', 'MA_20_lag3', 'MA_20_lag4', 'MA_20_lag5', 'MA_5_lag1', 'MA_5_lag2', 'MA_5_lag3', 'MA_5_lag4', 'MA_5_lag5', 'MA_20_lag1', 'MA_20_lag2', 'MA_20_lag3', 'MA_20_lag4', 'MA_20_lag5', 'sentiment_lag1', 'sentiment_lag2', 'sentiment_lag3', 'sentiment_lag4', 'sentiment_lag5', 'geo_keyword_hits_lag1', 'geo_keyword_hits_lag2', 'geo_keyword_hits_lag3', 'geo_keyword_hits_lag4', 'geo_keyword_hits_lag5', 'GPRD_percentile_lag1', 'GPRD_percentile_lag2', 'GPRD_percentile_lag3', 'GPRD_percentile_lag4', 'GPRD_percentile_lag5', 'GPRD_high_risk_lag1', 'GPRD_high_risk_lag2', 'GPRD_high_risk_lag3', 'GPRD_high_risk_lag4', 'GPRD_high_risk_lag5', 'GPRD_change_ma5_lag1', 'GPRD_change_ma5_lag2', 'GPRD_change_ma5_lag3', 'GPRD_change_ma5_lag4', 'GPRD_change_ma5_lag5']\n",
      "\n",
      "================================================================================\n",
      "Processing: Lithium (GPRD: OFF)\n",
      "================================================================================\n",
      "Renamed Close_LIT -> Close\n",
      "Features (40): ['Return_lag1', 'Return_lag2', 'Return_lag3']... (showing first 3)\n",
      "Clean data shape: (2847, 43)\n",
      "Final feature count: 40\n",
      "Train: 1839 samples | Test: 1008 samples\n",
      "Using 40 features for training\n",
      "DEBUG: train_df has 43 columns\n",
      "DEBUG: Requesting 40 features\n",
      "DEBUG: train_subset shape BEFORE scaling: (1839, 60)\n",
      "DEBUG: Scaled array shape: (1839, 60)\n",
      "DEBUG: Number of column names: 40\n",
      "ERROR: Shape mismatch! Array has 60 cols but 40 names\n",
      "Available features: ['Return_lag1', 'Return_lag2', 'Return_lag3', 'Return_lag4', 'Return_lag5', 'Vol_5_lag1', 'Vol_5_lag2', 'Vol_5_lag3', 'Vol_5_lag4', 'Vol_5_lag5', 'MA_5_lag1', 'MA_5_lag2', 'MA_5_lag3', 'MA_5_lag4', 'MA_5_lag5', 'MA_20_lag1', 'MA_20_lag2', 'MA_20_lag3', 'MA_20_lag4', 'MA_20_lag5', 'MA_5_lag1', 'MA_5_lag2', 'MA_5_lag3', 'MA_5_lag4', 'MA_5_lag5', 'MA_20_lag1', 'MA_20_lag2', 'MA_20_lag3', 'MA_20_lag4', 'MA_20_lag5', 'sentiment_lag1', 'sentiment_lag2', 'sentiment_lag3', 'sentiment_lag4', 'sentiment_lag5', 'geo_keyword_hits_lag1', 'geo_keyword_hits_lag2', 'geo_keyword_hits_lag3', 'geo_keyword_hits_lag4', 'geo_keyword_hits_lag5']\n",
      "\n",
      "================================================================================\n",
      "Processing: Lithium (GPRD: ON)\n",
      "================================================================================\n",
      "Renamed Close_LIT -> Close\n",
      "Features (55): ['Return_lag1', 'Return_lag2', 'Return_lag3']... (showing first 3)\n",
      "Clean data shape: (2847, 58)\n",
      "Final feature count: 55\n",
      "Train: 1839 samples | Test: 1008 samples\n",
      "Using 55 features for training\n",
      "DEBUG: train_df has 58 columns\n",
      "DEBUG: Requesting 55 features\n",
      "DEBUG: train_subset shape BEFORE scaling: (1839, 75)\n",
      "DEBUG: Scaled array shape: (1839, 75)\n",
      "DEBUG: Number of column names: 55\n",
      "ERROR: Shape mismatch! Array has 75 cols but 55 names\n",
      "Available features: ['Return_lag1', 'Return_lag2', 'Return_lag3', 'Return_lag4', 'Return_lag5', 'Vol_5_lag1', 'Vol_5_lag2', 'Vol_5_lag3', 'Vol_5_lag4', 'Vol_5_lag5', 'MA_5_lag1', 'MA_5_lag2', 'MA_5_lag3', 'MA_5_lag4', 'MA_5_lag5', 'MA_20_lag1', 'MA_20_lag2', 'MA_20_lag3', 'MA_20_lag4', 'MA_20_lag5', 'MA_5_lag1', 'MA_5_lag2', 'MA_5_lag3', 'MA_5_lag4', 'MA_5_lag5', 'MA_20_lag1', 'MA_20_lag2', 'MA_20_lag3', 'MA_20_lag4', 'MA_20_lag5', 'sentiment_lag1', 'sentiment_lag2', 'sentiment_lag3', 'sentiment_lag4', 'sentiment_lag5', 'geo_keyword_hits_lag1', 'geo_keyword_hits_lag2', 'geo_keyword_hits_lag3', 'geo_keyword_hits_lag4', 'geo_keyword_hits_lag5', 'GPRD_percentile_lag1', 'GPRD_percentile_lag2', 'GPRD_percentile_lag3', 'GPRD_percentile_lag4', 'GPRD_percentile_lag5', 'GPRD_high_risk_lag1', 'GPRD_high_risk_lag2', 'GPRD_high_risk_lag3', 'GPRD_high_risk_lag4', 'GPRD_high_risk_lag5', 'GPRD_change_ma5_lag1', 'GPRD_change_ma5_lag2', 'GPRD_change_ma5_lag3', 'GPRD_change_ma5_lag4', 'GPRD_change_ma5_lag5']\n",
      "\n",
      "================================================================================\n",
      "SUMMARY: GPRD Impact Analysis\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['commodity', 'use_gprd', 'test_r2', 'sharpe', 'alpha',\\n       'cumulative_return'],\\n      dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 42\u001B[39m\n\u001B[32m     40\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mSUMMARY: GPRD Impact Analysis\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     41\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33m=\u001B[39m\u001B[33m\"\u001B[39m*\u001B[32m80\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m42\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[43mresults_df\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mcommodity\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43muse_gprd\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mtest_r2\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43msharpe\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43malpha\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mcumulative_return\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m)\n\u001B[32m     44\u001B[39m \u001B[38;5;66;03m# Statistical comparison\u001B[39;00m\n\u001B[32m     45\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m + \u001B[33m\"\u001B[39m\u001B[33m=\u001B[39m\u001B[33m\"\u001B[39m*\u001B[32m80\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001B[39m, in \u001B[36mDataFrame.__getitem__\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   4111\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m is_iterator(key):\n\u001B[32m   4112\u001B[39m         key = \u001B[38;5;28mlist\u001B[39m(key)\n\u001B[32m-> \u001B[39m\u001B[32m4113\u001B[39m     indexer = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_get_indexer_strict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcolumns\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m[\u001B[32m1\u001B[39m]\n\u001B[32m   4115\u001B[39m \u001B[38;5;66;03m# take() does not accept boolean indexers\u001B[39;00m\n\u001B[32m   4116\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(indexer, \u001B[33m\"\u001B[39m\u001B[33mdtype\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) == \u001B[38;5;28mbool\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001B[39m, in \u001B[36mIndex._get_indexer_strict\u001B[39m\u001B[34m(self, key, axis_name)\u001B[39m\n\u001B[32m   6209\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   6210\u001B[39m     keyarr, indexer, new_indexer = \u001B[38;5;28mself\u001B[39m._reindex_non_unique(keyarr)\n\u001B[32m-> \u001B[39m\u001B[32m6212\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_raise_if_missing\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkeyarr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   6214\u001B[39m keyarr = \u001B[38;5;28mself\u001B[39m.take(indexer)\n\u001B[32m   6215\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, Index):\n\u001B[32m   6216\u001B[39m     \u001B[38;5;66;03m# GH 42790 - Preserve name from an Index\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6261\u001B[39m, in \u001B[36mIndex._raise_if_missing\u001B[39m\u001B[34m(self, key, indexer, axis_name)\u001B[39m\n\u001B[32m   6259\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m nmissing:\n\u001B[32m   6260\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m nmissing == \u001B[38;5;28mlen\u001B[39m(indexer):\n\u001B[32m-> \u001B[39m\u001B[32m6261\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mNone of [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m] are in the [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00maxis_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m]\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   6263\u001B[39m     not_found = \u001B[38;5;28mlist\u001B[39m(ensure_index(key)[missing_mask.nonzero()[\u001B[32m0\u001B[39m]].unique())\n\u001B[32m   6264\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnot_found\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m not in index\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mKeyError\u001B[39m: \"None of [Index(['commodity', 'use_gprd', 'test_r2', 'sharpe', 'alpha',\\n       'cumulative_return'],\\n      dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "80795772e4a87060"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
