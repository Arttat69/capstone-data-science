{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 0: Overview\n",
    "In this notebook, we:\n",
    "- Load processed commodity data and merged GPR from previous notebook.\n",
    "- Preprocess Global News Dataset: clean, extract geopolitical keyword counts per day.\n",
    "- Merge news features with commodity + GPR data.\n",
    " - Perform EDA: plots, correlations, distributions, stationarity tests.\n",
    " - Add advanced features (lags, rolling averages, event dummy).\n",
    " - Save final merged datasets and figures."
   ],
   "id": "41d8b6cb6344cd42"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 1: Import Libraries and Set Paths",
   "id": "36fcc4067889b8cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from statsmodels.tsa.stattools import adfuller  # For stationarity test\n",
    "# Optional: for sentiment (install if needed:\n",
    "!pip install textblob\n",
    "from textblob import TextBlob  # Uncomment after install\n",
    "# Paths (adjust if needed)\n",
    "ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "DATA_DIR = os.path.join(ROOT, \"data\")\n",
    "FIGURES_DIR = os.path.join(ROOT, \"figures\")\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)"
   ],
   "id": "86386d9ea8ebe6c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 2: Load Processed Data from Previous Notebook\n",
    "Load merged commodities with GPR (assume saved from prev notebook)"
   ],
   "id": "653526d74b268318"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "merged_gold = pd.read_csv(os.path.join(DATA_DIR, \"merged_gold.csv\"))\n",
    "merged_wti = pd.read_csv(os.path.join(DATA_DIR, \"merged_wti.csv\"))\n",
    "merged_wheat = pd.read_csv(os.path.join(DATA_DIR, \"merged_wheat.csv\"))\n",
    "\n",
    "# Ensure 'Date' is datetime\n",
    "for df in [merged_gold, merged_wti, merged_wheat]:\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "print(\"Loaded merged shapes:\", merged_gold.shape, merged_wti.shape, merged_wheat.shape)"
   ],
   "id": "834778f79a5c54da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 3: Load and Preprocess Global News Dataset",
   "id": "91f8610bef5235c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "news_path = os.path.join(DATA_DIR, \"data.csv\")  # From Kaggle download\n",
    "news_df = pd.read_csv(news_path)\n",
    "\n",
    "# Assume columns: 'published_at', 'title', 'short_description', 'source_name', etc.\n",
    "# Convert 'published_at' to datetime and extract date\n",
    "news_df['published_at'] = pd.to_datetime(news_df['published_at'])\n",
    "news_df['date'] = news_df['published_at'].dt.date  # Group by date\n",
    "\n",
    "# Clean text: lowercase titles and descriptions\n",
    "news_df['title'] = news_df['title'].str.lower().fillna('')\n",
    "news_df['short_description'] = news_df['short_description'].str.lower().fillna('')\n",
    "\n",
    "# Expanded keywords for robustness\n",
    "keywords = [\n",
    "    'war', 'sanctions', 'conflict', 'geopolitical', 'tension', 'embargo', 'crisis', 'invasion',\n",
    "    'terrorism', 'opec', 'blockade', 'dispute', 'escalation', 'hostility', 'unrest', 'strike',\n",
    "    'alliance', 'treaty', 'summit', 'diplomacy', 'opep', 'Iran', 'Syria', 'Lybia', 'North Korea'  # Added more for geopolitical context\n",
    "]\n",
    "\n",
    "# Create indicator for geopolitical news (1 if any keyword in title or desc)\n",
    "news_df['is_geopolitical'] = news_df.apply(\n",
    "    lambda row: any(kw in row['title'] or kw in row['short_description'] for kw in keywords), axis=1\n",
    ")\n",
    "\n",
    "Bonus: Sentiment on geopolitical articles (uncomment after installing TextBlob)\n",
    "def get_sentiment(text):\n",
    "     return TextBlob(text).sentiment.polarity if text else 0\n",
    " news_df['sentiment'] = news_df.apply(lambda row: get_sentiment(row['title'] + ' ' + row['short_description']) if row['is_geopolitical'] else np.nan, axis=1)\n",
    "\n",
    "# Aggregate per day: count of geopolitical articles (and mean sentiment if added)\n",
    "news_agg = news_df.groupby('date').agg(\n",
    "    geo_news_count=('is_geopolitical', 'sum'),\n",
    "     geo_avg_sentiment=('sentiment', 'mean')  # Uncomment if using sentiment\n",
    ").reset_index()\n",
    "\n",
    "# Convert date to datetime for merging\n",
    "news_agg['date'] = pd.to_datetime(news_agg['date'])\n",
    "news_agg.rename(columns={'date': 'Date'}, inplace=True)\n",
    "\n",
    "print(\"News aggregate preview:\")\n",
    "print(news_agg.head())\n",
    "\n",
    "# Save processed news for reference\n",
    "news_agg.to_csv(os.path.join(DATA_DIR, \"news_processed.csv\"), index=False)"
   ],
   "id": "8db408fce3ad8243"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 4: Merge News with Commodity + GPR Data\n",
    "Merge on 'Date' (left join to keep all trading days; fill missing with 0)"
   ],
   "id": "62da9f8ce7a48abd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def merge_with_news(df):\n",
    "    merged = pd.merge(df, news_agg, on='Date', how='left')\n",
    "    merged['geo_news_count'] = merged['geo_news_count'].fillna(0)\n",
    "    # merged['geo_avg_sentiment'] = merged['geo_avg_sentiment'].fillna(0)  # If using\n",
    "    return merged\n",
    "\n",
    "merged_gold_with_news = merge_with_news(merged_gold)\n",
    "merged_wti_with_news = merge_with_news(merged_wti)\n",
    "merged_wheat_with_news = merge_with_news(merged_wheat)\n",
    "\n",
    "# Save merged datasets\n",
    "merged_gold_with_news.to_csv(os.path.join(DATA_DIR, \"merged_gold_with_news.csv\"), index=False)\n",
    "merged_wti_with_news.to_csv(os.path.join(DATA_DIR, \"merged_wti_with_news.csv\"), index=False)\n",
    "merged_wheat_with_news.to_csv(os.path.join(DATA_DIR, \"merged_wheat_with_news.csv\"), index=False)\n",
    "\n",
    "print(\"Merged with news shapes:\", merged_gold_with_news.shape, merged_wti_with_news.shape, merged_wheat_with_news.shape)"
   ],
   "id": "e078de9b8f203d6f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 5: Feature Engineering (Step 3 from Plan)\n",
    " Price returns already in data ('Return')\n",
    " Rolling averages and volatility (5-day already in; add 30-day)\n",
    " Lag features (past returns; add 1-day)\n",
    " Geopolitical indicators: GPRD (already in), news count (added), event dummy (from 'EVENT')"
   ],
   "id": "743d95b971da54fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for df in [merged_gold_with_news, merged_wti_with_news, merged_wheat_with_news]:\n",
    "    # Rolling volatility (30-day)\n",
    "    df['vol_30'] = df['Return'].rolling(window=30).std()\n",
    "\n",
    "    # Lag features\n",
    "    df['return_lag1'] = df['Return'].shift(1)\n",
    "    df['gpr_lag1'] = df['GPRD'].shift(1)  # GPR lag\n",
    "    df['news_count_lag1'] = df['geo_news_count'].shift(1)  # News lag for delayed reactions\n",
    "\n",
    "    # Event dummy (1 if EVENT not NaN)\n",
    "    df['event_dummy'] = df['EVENT'].notna().astype(int)\n",
    "\n",
    "# Resave after feature engineering\n",
    "merged_gold_with_news.to_csv(os.path.join(DATA_DIR, \"merged_gold_with_news.csv\"), index=False)\n",
    "merged_wti_with_news.to_csv(os.path.join(DATA_DIR, \"merged_wti_with_news.csv\"), index=False)\n",
    "merged_wheat_with_news.to_csv(os.path.join(DATA_DIR, \"merged_wheat_with_news.csv\"), index=False)"
   ],
   "id": "cce15b11994da2bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 6: Exploratory Data Analysis (EDA) - Week 9\n",
    "6.1: Time Series Plots (Prices with GPR and News Overlaid)"
   ],
   "id": "5a2f464b5183ab22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_time_series(df, commodity, price_col='Close', gpr_col='GPRD', news_col='geo_news_count'):\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Price on left axis (adjust price_col based on your flattened columns, e.g., 'Close_GC=F')\n",
    "    ax1.plot(df['Date'], df[price_col], color='blue', label=f'{commodity} Price')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel(f'{commodity} Price', color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "    # GPR on right axis\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(df['Date'], df[gpr_col], color='red', label='GPR Index', alpha=0.7)\n",
    "    ax2.set_ylabel('GPR Index', color='red')\n",
    "    ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "    # News count as scatter (for highlights)\n",
    "    ax2.scatter(df['Date'], df[news_col] * 10, color='green', label='Geo News Count (scaled)', s=10, alpha=0.5)  # Scaled for visibility\n",
    "\n",
    "    fig.suptitle(f'{commodity} Price vs. Geopolitical Risk and News Counts')\n",
    "    fig.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, f'{commodity.lower()}_time_series.png'))\n",
    "    plt.show()\n",
    "\n",
    "# Generate plots (adjust price_col if needed based on your column names)\n",
    "plot_time_series(merged_gold_with_news, 'Gold', price_col='Close_GC=F')\n",
    "plot_time_series(merged_wti_with_news, 'WTI Oil', price_col='Close_CL=F')\n",
    "plot_time_series(merged_wheat_with_news, 'Wheat', price_col='Close_ZW=F')"
   ],
   "id": "53a0628c963a18b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " 6.2: Correlations (Table)",
   "id": "70cee22789fc77cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_correlations(df, key_cols=['Return', 'GPRD', 'GPRD_ACT', 'GPRD_THREAT', 'geo_news_count', 'event_dummy']):\n",
    "    corr = df[key_cols].corr()\n",
    "    print(f\"Correlation Matrix for {commodity}:\\n{corr}\")\n",
    "    # Save as heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(corr, cmap='coolwarm', interpolation='none')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(len(key_cols)), key_cols, rotation=45)\n",
    "    plt.yticks(range(len(key_cols)), key_cols)\n",
    "    plt.title('Correlation Heatmap')\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, 'correlation_heatmap.png'))\n",
    "    plt.show()\n",
    "\n",
    "# Example for Gold (repeat for others if needed)\n",
    "get_correlations(merged_gold_with_news)"
   ],
   "id": "40bcde47451108dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "6.3: Distributions (Histograms)",
   "id": "2f0655e2e314079f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_distributions(df, cols=['Return', 'GPRD', 'geo_news_count']):\n",
    "    fig, axes = plt.subplots(1, len(cols), figsize=(15, 5))\n",
    "    for i, col in enumerate(cols):\n",
    "        axes[i].hist(df[col].dropna(), bins=50)\n",
    "        axes[i].set_title(f'Distribution of {col}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, 'distributions.png'))\n",
    "    plt.show()\n",
    "\n",
    "plot_distributions(merged_gold_with_news)  # Repeat for others if needed"
   ],
   "id": "ca28a5a1f5402dde"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "6.4: Stationarity Test (ADF for prices/returns)",
   "id": "cd51c914ddcf56f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def test_stationarity(series, name):\n",
    "    result = adfuller(series.dropna())\n",
    "    print(f'ADF Statistic for {name}: {result[0]}')\n",
    "    print(f'p-value: {result[1]}')\n",
    "    if result[1] <= 0.05:\n",
    "        print(f\"{name} is stationary (reject null).\")\n",
    "    else:\n",
    "        print(f\"{name} is non-stationary.\")\n",
    "\n",
    "# Test on Gold prices and returns (example; repeat for others)\n",
    "test_stationarity(merged_gold_with_news['Close_GC=F'], 'Gold Price')  # Likely non-stationary\n",
    "test_stationarity(merged_gold_with_news['Return'], 'Gold Returns')  # Likely stationary"
   ],
   "id": "a903d30c1f23acfc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
