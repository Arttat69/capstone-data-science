{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 0: Overview\n",
    "In this notebook, we:\n",
    "- Load processed commodity data and merged GPR from previous notebook.\n",
    "- Preprocess Global News Dataset: clean, extract geopolitical keyword counts per day.\n",
    "- Merge news features with commodity + GPR data.\n",
    " - Perform EDA: plots, correlations, distributions, stationarity tests.\n",
    " - Add advanced features (lags, rolling averages, event dummy).\n",
    " - Save final merged datasets and figures."
   ],
   "id": "41d8b6cb6344cd42"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 1: Import Libraries and Set Paths",
   "id": "36fcc4067889b8cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T14:14:30.068880Z",
     "start_time": "2025-10-11T14:14:12.868216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "#from statsmodels.tsa.stattools import adfuller  # For stationarity test\n",
    "# Optional: for sentiment (install if needed:\n",
    "!pip install textblob\n",
    "from textblob import TextBlob  # Uncomment after install\n",
    "# Paths (adjust if needed)\n",
    "ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "DATA_DIR = os.path.join(os.getcwd(), \"data\")  # Points to notebooks/data\n",
    "FIGURES_DIR = os.path.join(ROOT, \"figures\")\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)"
   ],
   "id": "86386d9ea8ebe6c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in d:\\python\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in d:\\python\\lib\\site-packages (from textblob) (3.9.2)\n",
      "Requirement already satisfied: click in d:\\python\\lib\\site-packages (from nltk>=3.9->textblob) (8.3.0)\n",
      "Requirement already satisfied: joblib in d:\\python\\lib\\site-packages (from nltk>=3.9->textblob) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\python\\lib\\site-packages (from nltk>=3.9->textblob) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in d:\\python\\lib\\site-packages (from nltk>=3.9->textblob) (4.67.1)\n",
      "Requirement already satisfied: colorama in d:\\python\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T14:14:33.107550Z",
     "start_time": "2025-10-11T14:14:33.097935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Proposed ROOT:\", os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "print(\"Proposed DATA_DIR:\", os.path.join(os.path.abspath(os.path.join(os.getcwd(), \"..\")), \"data\"))\n",
    "\n",
    "# Check if data folder exists\n",
    "if os.path.exists(os.path.join(os.path.abspath(os.path.join(os.getcwd(), \"..\")), \"data\")):\n",
    "    print(\"Data folder exists. Contents:\", os.listdir(os.path.join(os.path.abspath(os.path.join(os.getcwd(), \"..\")), \"data\")))\n",
    "else:\n",
    "    print(\"Data folder does not existâ€”create it or adjust paths.\")"
   ],
   "id": "524e79c96acb1b83",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\taton\\PycharmProjects\\capstone-data-science\\notebooks\n",
      "Proposed ROOT: C:\\Users\\taton\\PycharmProjects\\capstone-data-science\n",
      "Proposed DATA_DIR: C:\\Users\\taton\\PycharmProjects\\capstone-data-science\\data\n",
      "Data folder exists. Contents: ['.gitkeep', 'lstm_reference']\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Step 2: Load Processed Data from Previous Notebook\n",
    "Load merged commodities with GPR (assume saved from prev notebook)"
   ],
   "id": "653526d74b268318"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T14:14:41.670190Z",
     "start_time": "2025-10-11T14:14:40.574834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "merged_gold = pd.read_csv(os.path.join(DATA_DIR, \"gold_merged.csv\"))\n",
    "merged_wti = pd.read_csv(os.path.join(DATA_DIR, \"wti_merged.csv\"))\n",
    "merged_wheat = pd.read_csv(os.path.join(DATA_DIR, \"wheat_merged.csv\"))\n",
    "\n",
    "# Ensure 'Date' is datetime\n",
    "for df in [merged_gold, merged_wti, merged_wheat]:\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "print(\"Loaded merged shapes:\", merged_gold.shape, merged_wti.shape, merged_wheat.shape)"
   ],
   "id": "834778f79a5c54da",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\taton\\\\PycharmProjects\\\\capstone-data-science\\\\data\\\\gold_merged.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m merged_gold = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m.\u001B[49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDATA_DIR\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mgold_merged.csv\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      2\u001B[39m merged_wti = pd.read_csv(os.path.join(DATA_DIR, \u001B[33m\"\u001B[39m\u001B[33mwti_merged.csv\u001B[39m\u001B[33m\"\u001B[39m))\n\u001B[32m      3\u001B[39m merged_wheat = pd.read_csv(os.path.join(DATA_DIR, \u001B[33m\"\u001B[39m\u001B[33mwheat_merged.csv\u001B[39m\u001B[33m\"\u001B[39m))\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001B[39m, in \u001B[36mread_csv\u001B[39m\u001B[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[39m\n\u001B[32m   1013\u001B[39m kwds_defaults = _refine_defaults_read(\n\u001B[32m   1014\u001B[39m     dialect,\n\u001B[32m   1015\u001B[39m     delimiter,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1022\u001B[39m     dtype_backend=dtype_backend,\n\u001B[32m   1023\u001B[39m )\n\u001B[32m   1024\u001B[39m kwds.update(kwds_defaults)\n\u001B[32m-> \u001B[39m\u001B[32m1026\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001B[39m, in \u001B[36m_read\u001B[39m\u001B[34m(filepath_or_buffer, kwds)\u001B[39m\n\u001B[32m    617\u001B[39m _validate_names(kwds.get(\u001B[33m\"\u001B[39m\u001B[33mnames\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[32m    619\u001B[39m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m620\u001B[39m parser = \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    622\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[32m    623\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001B[39m, in \u001B[36mTextFileReader.__init__\u001B[39m\u001B[34m(self, f, engine, **kwds)\u001B[39m\n\u001B[32m   1617\u001B[39m     \u001B[38;5;28mself\u001B[39m.options[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m] = kwds[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m   1619\u001B[39m \u001B[38;5;28mself\u001B[39m.handles: IOHandles | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1620\u001B[39m \u001B[38;5;28mself\u001B[39m._engine = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001B[39m, in \u001B[36mTextFileReader._make_engine\u001B[39m\u001B[34m(self, f, engine)\u001B[39m\n\u001B[32m   1878\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[32m   1879\u001B[39m         mode += \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1880\u001B[39m \u001B[38;5;28mself\u001B[39m.handles = \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1881\u001B[39m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1882\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1883\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mencoding\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1884\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcompression\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1885\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmemory_map\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1886\u001B[39m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1887\u001B[39m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mencoding_errors\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstrict\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1888\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstorage_options\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1889\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1890\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m.handles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1891\u001B[39m f = \u001B[38;5;28mself\u001B[39m.handles.handle\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python\\Lib\\site-packages\\pandas\\io\\common.py:873\u001B[39m, in \u001B[36mget_handle\u001B[39m\u001B[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[39m\n\u001B[32m    868\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m    869\u001B[39m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[32m    870\u001B[39m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[32m    871\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m ioargs.encoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs.mode:\n\u001B[32m    872\u001B[39m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m873\u001B[39m         handle = \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[32m    874\u001B[39m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    875\u001B[39m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    876\u001B[39m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[43mioargs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    877\u001B[39m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    878\u001B[39m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    879\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    880\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    881\u001B[39m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[32m    882\u001B[39m         handle = \u001B[38;5;28mopen\u001B[39m(handle, ioargs.mode)\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\taton\\\\PycharmProjects\\\\capstone-data-science\\\\data\\\\gold_merged.csv'"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 3: Load and Preprocess Global News Dataset",
   "id": "91f8610bef5235c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "news_path = os.path.join(DATA_DIR,\"Global News dataset\", \"data.csv\")  # From Kaggle download\n",
    "news_df = pd.read_csv(news_path)\n",
    "\n",
    "# Assume columns: 'published_at', 'title', 'short_description', 'source_name', etc.\n",
    "# Convert 'published_at' to datetime and extract date\n",
    "news_df['published_at'] = pd.to_datetime(news_df['published_at'])\n",
    "news_df['date'] = news_df['published_at'].dt.date  # Group by date\n",
    "\n",
    "# Clean text: lowercase titles and descriptions\n",
    "news_df['title'] = news_df['title'].str.lower().fillna('')\n",
    "news_df['short_description'] = news_df['short_description'].str.lower().fillna('')\n",
    "\n",
    "# Expanded keywords for robustness\n",
    "keywords = [\n",
    "    'war', 'sanctions', 'conflict', 'geopolitical', 'tension', 'embargo', 'crisis', 'invasion',\n",
    "    'terrorism', 'opec', 'blockade', 'dispute', 'escalation', 'hostility', 'unrest', 'strike',\n",
    "    'alliance', 'treaty', 'summit', 'diplomacy', 'opep', 'Iran', 'Syria', 'Lybia', 'North Korea'  # Added more for geopolitical context\n",
    "]\n",
    "\n",
    "# Create indicator for geopolitical news (1 if any keyword in title or desc)\n",
    "news_df['is_geopolitical'] = news_df.apply(\n",
    "    lambda row: any(kw in row['title'] or kw in row['short_description'] for kw in keywords), axis=1\n",
    ")\n",
    "\n",
    "Bonus: Sentiment on geopolitical articles (uncomment after installing TextBlob)\n",
    "def get_sentiment(text):\n",
    "     return TextBlob(text).sentiment.polarity if text else 0\n",
    " news_df['sentiment'] = news_df.apply(lambda row: get_sentiment(row['title'] + ' ' + row['short_description']) if row['is_geopolitical'] else np.nan, axis=1)\n",
    "\n",
    "# Aggregate per day: count of geopolitical articles (and mean sentiment if added)\n",
    "news_agg = news_df.groupby('date').agg(\n",
    "    geo_news_count=('is_geopolitical', 'sum'),\n",
    "     geo_avg_sentiment=('sentiment', 'mean')  # Uncomment if using sentiment\n",
    ").reset_index()\n",
    "\n",
    "# Convert date to datetime for merging\n",
    "news_agg['date'] = pd.to_datetime(news_agg['date'])\n",
    "news_agg.rename(columns={'date': 'Date'}, inplace=True)\n",
    "\n",
    "print(\"News aggregate preview:\")\n",
    "print(news_agg.head())\n",
    "\n",
    "# Save processed news for reference\n",
    "news_agg.to_csv(os.path.join(DATA_DIR, \"news_processed.csv\"), index=False)"
   ],
   "id": "8db408fce3ad8243"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 4: Merge News with Commodity + GPR Data\n",
    "Merge on 'Date' (left join to keep all trading days; fill missing with 0)"
   ],
   "id": "62da9f8ce7a48abd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def merge_with_news(df):\n",
    "    merged = pd.merge(df, news_agg, on='Date', how='left')\n",
    "    merged['geo_news_count'] = merged['geo_news_count'].fillna(0)\n",
    "    # merged['geo_avg_sentiment'] = merged['geo_avg_sentiment'].fillna(0)  # If using\n",
    "    return merged\n",
    "\n",
    "merged_gold_with_news = merge_with_news(merged_gold)\n",
    "merged_wti_with_news = merge_with_news(merged_wti)\n",
    "merged_wheat_with_news = merge_with_news(merged_wheat)\n",
    "\n",
    "# Save merged datasets\n",
    "merged_gold_with_news.to_csv(os.path.join(DATA_DIR, \"merged_gold_with_news.csv\"), index=False)\n",
    "merged_wti_with_news.to_csv(os.path.join(DATA_DIR, \"merged_wti_with_news.csv\"), index=False)\n",
    "merged_wheat_with_news.to_csv(os.path.join(DATA_DIR, \"merged_wheat_with_news.csv\"), index=False)\n",
    "\n",
    "print(\"Merged with news shapes:\", merged_gold_with_news.shape, merged_wti_with_news.shape, merged_wheat_with_news.shape)"
   ],
   "id": "e078de9b8f203d6f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 5: Feature Engineering (Step 3 from Plan)\n",
    " Price returns already in data ('Return')\n",
    " Rolling averages and volatility (5-day already in; add 30-day)\n",
    " Lag features (past returns; add 1-day)\n",
    " Geopolitical indicators: GPRD (already in), news count (added), event dummy (from 'EVENT')"
   ],
   "id": "743d95b971da54fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for df in [merged_gold_with_news, merged_wti_with_news, merged_wheat_with_news]:\n",
    "    # Rolling volatility (30-day)\n",
    "    df['vol_30'] = df['Return'].rolling(window=30).std()\n",
    "\n",
    "    # Lag features\n",
    "    df['return_lag1'] = df['Return'].shift(1)\n",
    "    df['gpr_lag1'] = df['GPRD'].shift(1)  # GPR lag\n",
    "    df['news_count_lag1'] = df['geo_news_count'].shift(1)  # News lag for delayed reactions\n",
    "\n",
    "    # Event dummy (1 if EVENT not NaN)\n",
    "    df['event_dummy'] = df['EVENT'].notna().astype(int)\n",
    "\n",
    "# Resave after feature engineering\n",
    "merged_gold_with_news.to_csv(os.path.join(DATA_DIR, \"merged_gold_with_news.csv\"), index=False)\n",
    "merged_wti_with_news.to_csv(os.path.join(DATA_DIR, \"merged_wti_with_news.csv\"), index=False)\n",
    "merged_wheat_with_news.to_csv(os.path.join(DATA_DIR, \"merged_wheat_with_news.csv\"), index=False)"
   ],
   "id": "cce15b11994da2bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 6: Exploratory Data Analysis (EDA) - Week 9\n",
    "6.1: Time Series Plots (Prices with GPR and News Overlaid)"
   ],
   "id": "5a2f464b5183ab22"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T12:38:23.455087Z",
     "start_time": "2025-10-13T12:38:23.062788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_time_series(df, commodity, price_col='Close', gpr_col='GPRD', news_col='geo_news_count'):\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Price on left axis (adjust price_col based on your flattened columns, e.g., 'Close_GC=F')\n",
    "    ax1.plot(df['Date'], df[price_col], color='blue', label=f'{commodity} Price')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel(f'{commodity} Price', color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "    # GPR on right axis\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(df['Date'], df[gpr_col], color='red', label='GPR Index', alpha=0.7)\n",
    "    ax2.set_ylabel('GPR Index', color='red')\n",
    "    ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "    # News count as scatter (for highlights)\n",
    "    ax2.scatter(df['Date'], df[news_col] * 10, color='green', label='Geo News Count (scaled)', s=10, alpha=0.5)  # Scaled for visibility\n",
    "\n",
    "    fig.suptitle(f'{commodity} Price vs. Geopolitical Risk and News Counts')\n",
    "    fig.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, f'{commodity.lower()}_time_series.png'))\n",
    "    plt.show()\n",
    "\n",
    "# Generate plots (adjust price_col if needed based on your column names)\n",
    "plot_time_series(merged_gold_with_news, 'Gold', price_col='Close_GC=F')\n",
    "plot_time_series(merged_wti_with_news, 'WTI Oil', price_col='Close_CL=F')\n",
    "plot_time_series(merged_wheat_with_news, 'Wheat', price_col='Close_ZW=F')"
   ],
   "id": "53a0628c963a18b4",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_gold_with_news' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 26\u001B[39m\n\u001B[32m     23\u001B[39m     plt.show()\n\u001B[32m     25\u001B[39m \u001B[38;5;66;03m# Generate plots (adjust price_col if needed based on your column names)\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m26\u001B[39m plot_time_series(\u001B[43mmerged_gold_with_news\u001B[49m, \u001B[33m'\u001B[39m\u001B[33mGold\u001B[39m\u001B[33m'\u001B[39m, price_col=\u001B[33m'\u001B[39m\u001B[33mClose_GC=F\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m     27\u001B[39m plot_time_series(merged_wti_with_news, \u001B[33m'\u001B[39m\u001B[33mWTI Oil\u001B[39m\u001B[33m'\u001B[39m, price_col=\u001B[33m'\u001B[39m\u001B[33mClose_CL=F\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m     28\u001B[39m plot_time_series(merged_wheat_with_news, \u001B[33m'\u001B[39m\u001B[33mWheat\u001B[39m\u001B[33m'\u001B[39m, price_col=\u001B[33m'\u001B[39m\u001B[33mClose_ZW=F\u001B[39m\u001B[33m'\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'merged_gold_with_news' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " 6.2: Correlations (Table)",
   "id": "70cee22789fc77cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_correlations(df, key_cols=['Return', 'GPRD', 'GPRD_ACT', 'GPRD_THREAT', 'geo_news_count', 'event_dummy']):\n",
    "    corr = df[key_cols].corr()\n",
    "    print(f\"Correlation Matrix for {commodity}:\\n{corr}\")\n",
    "    # Save as heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(corr, cmap='coolwarm', interpolation='none')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(len(key_cols)), key_cols, rotation=45)\n",
    "    plt.yticks(range(len(key_cols)), key_cols)\n",
    "    plt.title('Correlation Heatmap')\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, 'correlation_heatmap.png'))\n",
    "    plt.show()\n",
    "\n",
    "# Example for Gold (repeat for others if needed)\n",
    "get_correlations(merged_gold_with_news)"
   ],
   "id": "40bcde47451108dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "6.3: Distributions (Histograms)",
   "id": "2f0655e2e314079f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_distributions(df, cols=['Return', 'GPRD', 'geo_news_count']):\n",
    "    fig, axes = plt.subplots(1, len(cols), figsize=(15, 5))\n",
    "    for i, col in enumerate(cols):\n",
    "        axes[i].hist(df[col].dropna(), bins=50)\n",
    "        axes[i].set_title(f'Distribution of {col}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(FIGURES_DIR, 'distributions.png'))\n",
    "    plt.show()\n",
    "\n",
    "plot_distributions(merged_gold_with_news)  # Repeat for others if needed"
   ],
   "id": "ca28a5a1f5402dde"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "6.4: Stationarity Test (ADF for prices/returns)",
   "id": "cd51c914ddcf56f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def test_stationarity(series, name):\n",
    "    result = adfuller(series.dropna())\n",
    "    print(f'ADF Statistic for {name}: {result[0]}')\n",
    "    print(f'p-value: {result[1]}')\n",
    "    if result[1] <= 0.05:\n",
    "        print(f\"{name} is stationary (reject null).\")\n",
    "    else:\n",
    "        print(f\"{name} is non-stationary.\")\n",
    "\n",
    "# Test on Gold prices and returns (example; repeat for others)\n",
    "test_stationarity(merged_gold_with_news['Close_GC=F'], 'Gold Price')  # Likely non-stationary\n",
    "test_stationarity(merged_gold_with_news['Return'], 'Gold Returns')  # Likely stationary"
   ],
   "id": "a903d30c1f23acfc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
