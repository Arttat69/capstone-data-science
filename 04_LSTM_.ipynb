{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-20T17:15:55.050127Z",
     "start_time": "2025-11-20T17:15:34.059595Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, mean_absolute_error, r2_score\n",
    "from sklearn.cluster import KMeans\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import LSTM, Dense, Input, Dropout, Bidirectional, Layer\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.optimizers import Adam\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set paths (same as previous)\n",
    "ROOT = os.getcwd()\n",
    "DATA_DIR = os.path.join(ROOT, \"data\")\n",
    "ENRICHED_DIR = os.path.join(DATA_DIR, \"enriched\")\n",
    "MODEL_RESULTS_DIR = os.path.join(DATA_DIR, 'model_results')\n",
    "os.makedirs(MODEL_RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Commodity names\n",
    "commodities = [\"Gold\", \"WTI\", \"Wheat\", \"NaturalGas\", \"Copper\", \"Lithium\"]\n",
    "\n",
    "# Load enriched data\n",
    "merged_data = {}\n",
    "for name in commodities:\n",
    "    fname = f\"{name.lower()}_enriched.csv\"\n",
    "    path = os.path.join(ENRICHED_DIR, fname)\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path)\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        merged_data[name] = df\n",
    "    else:\n",
    "        print(f\"Missing enriched file for {name}\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T17:15:55.135895Z",
     "start_time": "2025-11-20T17:15:55.117628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def add_lagged_features(df, feature_cols, max_lag=5):\n",
    "    \"\"\"Add lagged features with proper handling\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    for col in feature_cols:\n",
    "        if col in df_copy.columns:\n",
    "            for lag in range(1, max_lag + 1):\n",
    "                df_copy[f'{col}_lag{lag}'] = df_copy[col].shift(lag)\n",
    "    return df_copy\n",
    "\n",
    "def create_sequences(data, feature_cols, target_col, seq_length):\n",
    "    \"\"\"Create sequences for LSTM\"\"\"\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[feature_cols].iloc[i:i+seq_length].values\n",
    "        y = data[target_col].iloc[i+seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "def compute_max_drawdown(returns):\n",
    "    \"\"\"Calculate maximum drawdown from returns series\"\"\"\n",
    "    cumulative = np.cumprod(1 + returns)\n",
    "    running_max = np.maximum.accumulate(cumulative)\n",
    "    drawdown = (cumulative - running_max) / running_max\n",
    "    return np.min(drawdown)\n",
    "\n",
    "def backtest_volatility_strategy(vol_pred, vol_actual, returns, transaction_cost=0.001):\n",
    "    \"\"\"\n",
    "    Trading strategy based on volatility predictions\n",
    "\n",
    "    Strategy Logic:\n",
    "    - When predicted vol > 75th percentile: Reduce position (expect mean reversion)\n",
    "    - When predicted vol < 25th percentile: Increase position (expect trending)\n",
    "    - Use volatility forecast errors to adjust signals\n",
    "    \"\"\"\n",
    "    # Normalize predictions\n",
    "    vol_pred_norm = (vol_pred - np.mean(vol_pred)) / np.std(vol_pred)\n",
    "\n",
    "    # Generate signals based on volatility regime\n",
    "    high_vol_threshold = np.percentile(vol_pred, 75)\n",
    "    low_vol_threshold = np.percentile(vol_pred, 25)\n",
    "\n",
    "    signals = np.zeros(len(vol_pred))\n",
    "    signals[vol_pred > high_vol_threshold] = -0.5  # Reduce exposure in high vol\n",
    "    signals[vol_pred < low_vol_threshold] = 1.0    # Full exposure in low vol\n",
    "    signals[(vol_pred >= low_vol_threshold) & (vol_pred <= high_vol_threshold)] = 0.5\n",
    "\n",
    "    # Align returns (shift by 1 to avoid lookahead bias)\n",
    "    aligned_returns = returns[1:len(signals)+1]\n",
    "    signals = signals[:len(aligned_returns)]\n",
    "\n",
    "    # Calculate strategy returns\n",
    "    strategy_returns = signals * aligned_returns\n",
    "\n",
    "    # Apply transaction costs\n",
    "    position_changes = np.abs(np.diff(np.concatenate([[0], signals])))\n",
    "    transaction_costs = position_changes * transaction_cost\n",
    "    strategy_returns = strategy_returns - transaction_costs[:len(strategy_returns)]\n",
    "\n",
    "    # Metrics\n",
    "    sharpe = np.mean(strategy_returns) / (np.std(strategy_returns) + 1e-10) * np.sqrt(252)\n",
    "    cumulative_return = np.prod(1 + strategy_returns) - 1\n",
    "    max_dd = compute_max_drawdown(strategy_returns)\n",
    "    win_rate = np.mean(strategy_returns > 0)\n",
    "\n",
    "    # Buy & Hold comparison\n",
    "    bh_returns = aligned_returns\n",
    "    bh_sharpe = np.mean(bh_returns) / (np.std(bh_returns) + 1e-10) * np.sqrt(252)\n",
    "\n",
    "    return {\n",
    "        'sharpe': sharpe,\n",
    "        'cumulative_return': cumulative_return * 100,\n",
    "        'max_drawdown': max_dd * 100,\n",
    "        'win_rate': win_rate * 100,\n",
    "        'bh_sharpe': bh_sharpe,\n",
    "        'alpha': sharpe - bh_sharpe,\n",
    "        'avg_return_pct': np.mean(strategy_returns) * 100,\n",
    "        'volatility': np.std(strategy_returns) * 100\n",
    "    }\n",
    "\n",
    "def build_robust_lstm(input_shape, use_bidirectional=False):\n",
    "    \"\"\"\n",
    "    Build LSTM with regularization to prevent overfitting\n",
    "\n",
    "    Key changes from original:\n",
    "    - Smaller network (less prone to memorization)\n",
    "    - L1+L2 regularization\n",
    "    - Huber loss (robust to outliers)\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        LSTM(32, return_sequences=True if use_bidirectional else False,\n",
    "             kernel_regularizer=l1_l2(l1=0.0001, l2=0.001)),\n",
    "        Dropout(0.2),\n",
    "    ])\n",
    "\n",
    "    if use_bidirectional:\n",
    "        model.add(LSTM(16, kernel_regularizer=l1_l2(l1=0.0001, l2=0.001)))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Huber loss is less sensitive to outliers than MSE\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='huber',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def prepare_gprd_features(df, gprd_col='GPRD', method='percentile'):\n",
    "    \"\"\"\n",
    "    Smart GPRD feature engineering to avoid noise\n",
    "\n",
    "    Issue: Raw GPRD may be too noisy for LSTM\n",
    "    Solution: Use regime indicators instead of raw values\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    if method == 'percentile':\n",
    "        # Convert to percentile ranks (0-100)\n",
    "        df_copy['GPRD_percentile'] = df_copy[gprd_col].rank(pct=True) * 100\n",
    "\n",
    "        # Binary high-risk regime\n",
    "        df_copy['GPRD_high_risk'] = (df_copy['GPRD_percentile'] > 75).astype(int)\n",
    "\n",
    "        # Changes in GPRD (captures shocks)\n",
    "        df_copy['GPRD_change'] = df_copy[gprd_col].diff()\n",
    "        df_copy['GPRD_change_ma5'] = df_copy['GPRD_change'].rolling(5).mean()\n",
    "\n",
    "    elif method == 'zscore':\n",
    "        # Z-score normalization\n",
    "        gprd_mean = df_copy[gprd_col].rolling(60).mean()\n",
    "        gprd_std = df_copy[gprd_col].rolling(60).std()\n",
    "        df_copy['GPRD_zscore'] = (df_copy[gprd_col] - gprd_mean) / (gprd_std + 1e-10)\n",
    "        df_copy['GPRD_zscore'] = df_copy['GPRD_zscore'].clip(-3, 3)  # Remove extreme outliers\n",
    "\n",
    "    return df_copy\n"
   ],
   "id": "64772a4d221d3a53",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T17:15:55.787288Z",
     "start_time": "2025-11-20T17:15:55.736670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ============================================================================\n",
    "# MAIN MODELING PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def run_enhanced_lstm_with_gprd(commodity_name, df, use_gprd=True):\n",
    "    \"\"\"\n",
    "    Run LSTM with optional GPRD integration and full backtesting\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Processing: {commodity_name} (GPRD: {'ON' if use_gprd else 'OFF'})\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    # Prepare data\n",
    "    df = df.copy()\n",
    "    df = df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "    # CRITICAL FIX: Detect and rename commodity-specific columns\n",
    "    close_col = None\n",
    "    for col in df.columns:\n",
    "        if 'Close' in col or 'close' in col:\n",
    "            close_col = col\n",
    "            break\n",
    "\n",
    "    if close_col is None:\n",
    "        print(f\"ERROR: No Close column found in {df.columns.tolist()}\")\n",
    "        return None\n",
    "\n",
    "    # Rename to standard name if needed\n",
    "    if close_col != 'Close':\n",
    "        df['Close'] = df[close_col]\n",
    "        print(f\"Renamed {close_col} -> Close\")\n",
    "\n",
    "    # Calculate Return and Vol_5 if missing\n",
    "    if 'Return' not in df.columns:\n",
    "        df['Return'] = df['Close'].pct_change()\n",
    "        print(\"Calculated Return from Close prices\")\n",
    "\n",
    "    if 'Vol_5' not in df.columns:\n",
    "        df['Vol_5'] = df['Return'].rolling(5).std()\n",
    "        print(\"Calculated Vol_5 from Returns\")\n",
    "\n",
    "    # Identify price column (could be 'Close', 'Price', or 'PRICE')\n",
    "    price_col = 'Close'  # We just ensured this exists above\n",
    "\n",
    "    # Add technical indicators if not present\n",
    "    if 'MA_5' not in df.columns:\n",
    "        df['MA_5'] = df['Close'].rolling(5).mean()\n",
    "    if 'MA_20' not in df.columns:\n",
    "        df['MA_20'] = df['Close'].rolling(20).mean()\n",
    "\n",
    "    # Feature engineering for GPRD\n",
    "    if use_gprd and 'GPRD' in df.columns:\n",
    "        df = prepare_gprd_features(df, method='percentile')\n",
    "        gprd_features = ['GPRD_percentile', 'GPRD_high_risk', 'GPRD_change_ma5']\n",
    "    else:\n",
    "        gprd_features = []\n",
    "\n",
    "    # Base features (always included)\n",
    "    base_features = ['Return', 'Vol_5', 'MA_5', 'MA_20']\n",
    "\n",
    "     # Add MA features only if they were successfully created\n",
    "    if 'MA_5' in df.columns and not df['MA_5'].isna().all():\n",
    "        base_features.append('MA_5')\n",
    "    if 'MA_20' in df.columns and not df['MA_20'].isna().all():\n",
    "        base_features.append('MA_20')\n",
    "\n",
    "    # Add sentiment if available\n",
    "    if 'sentiment' in df.columns:\n",
    "        df['sentiment'].fillna(0, inplace=True)\n",
    "        base_features.append('sentiment')\n",
    "\n",
    "    if 'geo_keyword_hits' in df.columns:\n",
    "        df['geo_keyword_hits'].fillna(0, inplace=True)\n",
    "        base_features.append('geo_keyword_hits')\n",
    "\n",
    "    # Create lagged features\n",
    "    features_to_lag = base_features + gprd_features\n",
    "    df = add_lagged_features(df, features_to_lag, max_lag=5)\n",
    "\n",
    "    # Construct feature list\n",
    "    feature_cols = []\n",
    "    for feat in features_to_lag:\n",
    "        feature_cols.extend([f'{feat}_lag{i}' for i in range(1, 6)])\n",
    "\n",
    "    # Remove features with too many NaNs\n",
    "    df_clean = df[['Date', 'Vol_5', 'Return'] + feature_cols].copy()\n",
    "    df_clean = df_clean.dropna()\n",
    "\n",
    "    print(f\"Features ({len(feature_cols)}): {feature_cols[:3]}... (showing first 3)\")\n",
    "    print(f\"Clean data shape: {df_clean.shape}\")\n",
    "\n",
    "    if len(df_clean) < 100:\n",
    "        print(f\"Insufficient data after cleaning. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    # Train/test split\n",
    "    split_date = pd.to_datetime('2018-01-01')\n",
    "    train_df = df_clean[pd.to_datetime(df_clean['Date']) < split_date].copy()\n",
    "    test_df = df_clean[pd.to_datetime(df_clean['Date']) >= split_date].copy()\n",
    "\n",
    "    print(f\"Train: {len(train_df)} samples | Test: {len(test_df)} samples\")\n",
    "\n",
    "    if len(train_df) < 50 or len(test_df) < 20:\n",
    "        print(\"Insufficient train/test data. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    # Target and features\n",
    "    target = 'Vol_5'\n",
    "\n",
    "    # Scaling with RobustScaler (less sensitive to outliers)\n",
    "    scaler = RobustScaler()\n",
    "    train_features_scaled = scaler.fit_transform(train_df[feature_cols])\n",
    "    test_features_scaled = scaler.transform(test_df[feature_cols])\n",
    "\n",
    "    train_scaled_df = pd.DataFrame(train_features_scaled, columns=feature_cols, index=train_df.index)\n",
    "    test_scaled_df = pd.DataFrame(test_features_scaled, columns=feature_cols, index=test_df.index)\n",
    "\n",
    "    train_scaled_df[target] = train_df[target].values\n",
    "    test_scaled_df[target] = test_df[target].values\n",
    "    train_scaled_df['Return'] = train_df['Return'].values\n",
    "    test_scaled_df['Return'] = test_df['Return'].values\n",
    "\n",
    "    # Create sequences\n",
    "    seq_length = 20\n",
    "    X_train_seq, y_train_seq = create_sequences(train_scaled_df, feature_cols, target, seq_length)\n",
    "    X_test_seq, y_test_seq = create_sequences(test_scaled_df, feature_cols, target, seq_length)\n",
    "\n",
    "    print(f\"Sequences - Train: {X_train_seq.shape} | Test: {X_test_seq.shape}\")\n",
    "\n",
    "    if len(X_train_seq) < 20 or len(X_test_seq) < 5:\n",
    "        print(\"Insufficient sequences. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    # Build and train model\n",
    "    model = build_robust_lstm(input_shape=(seq_length, len(feature_cols)))\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=0)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001, verbose=0)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        epochs=100,\n",
    "        batch_size=64,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_train = model.predict(X_train_seq, verbose=0).squeeze()\n",
    "    y_pred_test = model.predict(X_test_seq, verbose=0).squeeze()\n",
    "\n",
    "    # Evaluation metrics\n",
    "    train_r2 = r2_score(y_train_seq, y_pred_train)\n",
    "    test_r2 = r2_score(y_test_seq, y_pred_test)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test_seq, y_pred_test))\n",
    "    test_mae = mean_absolute_error(y_test_seq, y_pred_test)\n",
    "\n",
    "    print(f\"\\nModel Performance:\")\n",
    "    print(f\"  Train R²: {train_r2:.4f} | Test R²: {test_r2:.4f}\")\n",
    "    print(f\"  Test RMSE: {test_rmse:.4f} | Test MAE: {test_mae:.4f}\")\n",
    "\n",
    "    # Extract returns for backtesting\n",
    "    test_returns = test_scaled_df['Return'].iloc[seq_length:].values\n",
    "\n",
    "    # Backtest trading strategy\n",
    "    backtest_results = backtest_volatility_strategy(\n",
    "        y_pred_test, y_test_seq, test_returns, transaction_cost=0.001\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTrading Strategy Performance:\")\n",
    "    print(f\"  Sharpe Ratio: {backtest_results['sharpe']:.3f}\")\n",
    "    print(f\"  Alpha (vs B&H): {backtest_results['alpha']:.3f}\")\n",
    "    print(f\"  Cumulative Return: {backtest_results['cumulative_return']:.2f}%\")\n",
    "    print(f\"  Max Drawdown: {backtest_results['max_drawdown']:.2f}%\")\n",
    "    print(f\"  Win Rate: {backtest_results['win_rate']:.2f}%\")\n",
    "\n",
    "    # Compile results\n",
    "    results = {\n",
    "        'commodity': commodity_name,\n",
    "        'use_gprd': use_gprd,\n",
    "        'n_features': len(feature_cols),\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mae': test_mae,\n",
    "        **backtest_results\n",
    "    }\n",
    "    # Visualization\n",
    "    plot_results(commodity_name, y_test_seq, y_pred_test, test_returns, history, use_gprd)\n",
    "\n",
    "    return results\n",
    "\n",
    "def plot_results(commodity, y_true, y_pred, returns, history, use_gprd):\n",
    "    \"\"\"Create comprehensive visualization\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(f'{commodity} - LSTM Results (GPRD: {\"ON\" if use_gprd else \"OFF\"})', fontsize=16)\n",
    "\n",
    "    # Plot 1: Predictions vs Actual\n",
    "    axes[0, 0].plot(y_true[-200:], label='Actual Vol', alpha=0.7, linewidth=2)\n",
    "    axes[0, 0].plot(y_pred[-200:], label='Predicted Vol', alpha=0.7, linewidth=2)\n",
    "    axes[0, 0].set_title('Volatility Forecast (Last 200 days)')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "    # Plot 2: Training history\n",
    "    axes[0, 1].plot(history.history['loss'], label='Train Loss')\n",
    "    axes[0, 1].plot(history.history['val_loss'], label='Val Loss')\n",
    "    axes[0, 1].set_title('Training History')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "    # Plot 3: Residuals\n",
    "    residuals = y_true - y_pred\n",
    "    axes[1, 0].scatter(y_pred, residuals, alpha=0.3)\n",
    "    axes[1, 0].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[1, 0].set_title('Residual Plot')\n",
    "    axes[1, 0].set_xlabel('Predicted')\n",
    "    axes[1, 0].set_ylabel('Residuals')\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "    # Plot 4: Distribution comparison\n",
    "    axes[1, 1].hist(y_true, bins=30, alpha=0.5, label='Actual', density=True)\n",
    "    axes[1, 1].hist(y_pred, bins=30, alpha=0.5, label='Predicted', density=True)\n",
    "    axes[1, 1].set_title('Distribution Comparison')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(MODEL_RESULTS_DIR, f'{commodity}_lstm_{\"gprd\" if use_gprd else \"nogprd\"}.png'), dpi=150)\n",
    "    plt.show()\n"
   ],
   "id": "2091515121ac2177",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T17:15:58.511812Z",
     "start_time": "2025-11-20T17:15:55.871577Z"
    }
   },
   "cell_type": "code",
   "source": [
    " # ============================================================================\n",
    "# RUN EXPERIMENTS\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    print(\"Loading enriched data...\")\n",
    "    merged_data = {}\n",
    "    for name in commodities:\n",
    "        fname = f\"{name.lower()}_enriched.csv\"\n",
    "        path = os.path.join(ENRICHED_DIR, fname)\n",
    "        if os.path.exists(path):\n",
    "            df = pd.read_csv(path)\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "            merged_data[name] = df\n",
    "            print(f\"  Loaded {name}: {len(df)} rows, Columns: {df.columns.tolist()[:5]}...\")\n",
    "        else:\n",
    "            print(f\"  Missing: {name}\")\n",
    "\n",
    "    # Run experiments: WITH and WITHOUT GPRD\n",
    "    all_results = []\n",
    "\n",
    "    for commodity in merged_data.keys():\n",
    "        # Without GPRD\n",
    "        results_no_gprd = run_enhanced_lstm_with_gprd(commodity, merged_data[commodity], use_gprd=False)\n",
    "        if results_no_gprd:\n",
    "            all_results.append(results_no_gprd)\n",
    "\n",
    "        # With GPRD (if available)\n",
    "        if 'GPRD' in merged_data[commodity].columns:\n",
    "            results_with_gprd = run_enhanced_lstm_with_gprd(commodity, merged_data[commodity], use_gprd=True)\n",
    "            if results_with_gprd:\n",
    "                all_results.append(results_with_gprd)\n",
    "\n",
    "    # Save results\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_df.to_csv(os.path.join(MODEL_RESULTS_DIR, 'lstm_comparison_gprd.csv'), index=False)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SUMMARY: GPRD Impact Analysis\")\n",
    "    print(\"=\"*80)\n",
    "    print(results_df[['commodity', 'use_gprd', 'test_r2', 'sharpe', 'alpha', 'cumulative_return']])\n",
    "\n",
    "    # Statistical comparison\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Average Performance by GPRD Usage:\")\n",
    "    print(\"=\"*80)\n",
    "    print(results_df.groupby('use_gprd')[['test_r2', 'sharpe', 'alpha']].mean())"
   ],
   "id": "d752485d2b5a685c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading enriched data...\n",
      "  Loaded Gold: 5342 rows, Columns: ['Date', 'Close_GC=F', 'High_GC=F', 'Low_GC=F', 'Open_GC=F']...\n",
      "  Loaded WTI: 5351 rows, Columns: ['Date', 'Close_CL=F', 'High_CL=F', 'Low_CL=F', 'Open_CL=F']...\n",
      "  Loaded Wheat: 5367 rows, Columns: ['Date', 'Close_ZW=F', 'High_ZW=F', 'Low_ZW=F', 'Open_ZW=F']...\n",
      "  Loaded NaturalGas: 3694 rows, Columns: ['Date', 'Close_UNG', 'High_UNG', 'Low_UNG', 'Open_UNG']...\n",
      "  Loaded Copper: 5346 rows, Columns: ['Date', 'Close_HG=F', 'High_HG=F', 'Low_HG=F', 'Open_HG=F']...\n",
      "  Loaded Lithium: 2871 rows, Columns: ['Date', 'Close_LIT', 'High_LIT', 'Low_LIT', 'Open_LIT']...\n",
      "\n",
      "================================================================================\n",
      "Processing: Gold (GPRD: OFF)\n",
      "================================================================================\n",
      "Renamed Close_GC=F -> Close\n",
      "Features (40): ['Return_lag1', 'Return_lag2', 'Return_lag3']... (showing first 3)\n",
      "Clean data shape: (5318, 43)\n",
      "Train: 4311 samples | Test: 1007 samples\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (4311, 60), indices imply (4311, 40)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 25\u001B[39m\n\u001B[32m     21\u001B[39m all_results = []\n\u001B[32m     23\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m commodity \u001B[38;5;129;01min\u001B[39;00m merged_data.keys():\n\u001B[32m     24\u001B[39m     \u001B[38;5;66;03m# Without GPRD\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m25\u001B[39m     results_no_gprd = \u001B[43mrun_enhanced_lstm_with_gprd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommodity\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmerged_data\u001B[49m\u001B[43m[\u001B[49m\u001B[43mcommodity\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_gprd\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m     26\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m results_no_gprd:\n\u001B[32m     27\u001B[39m         all_results.append(results_no_gprd)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 115\u001B[39m, in \u001B[36mrun_enhanced_lstm_with_gprd\u001B[39m\u001B[34m(commodity_name, df, use_gprd)\u001B[39m\n\u001B[32m    112\u001B[39m train_features_scaled = scaler.fit_transform(train_df[feature_cols])\n\u001B[32m    113\u001B[39m test_features_scaled = scaler.transform(test_df[feature_cols])\n\u001B[32m--> \u001B[39m\u001B[32m115\u001B[39m train_scaled_df = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_features_scaled\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfeature_cols\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain_df\u001B[49m\u001B[43m.\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    116\u001B[39m test_scaled_df = pd.DataFrame(test_features_scaled, columns=feature_cols, index=test_df.index)\n\u001B[32m    118\u001B[39m train_scaled_df[target] = train_df[target].values\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python\\Lib\\site-packages\\pandas\\core\\frame.py:827\u001B[39m, in \u001B[36mDataFrame.__init__\u001B[39m\u001B[34m(self, data, index, columns, dtype, copy)\u001B[39m\n\u001B[32m    816\u001B[39m         mgr = dict_to_mgr(\n\u001B[32m    817\u001B[39m             \u001B[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001B[39;00m\n\u001B[32m    818\u001B[39m             \u001B[38;5;66;03m# attribute \"name\"\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    824\u001B[39m             copy=_copy,\n\u001B[32m    825\u001B[39m         )\n\u001B[32m    826\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m827\u001B[39m         mgr = \u001B[43mndarray_to_mgr\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    828\u001B[39m \u001B[43m            \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    829\u001B[39m \u001B[43m            \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    830\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    831\u001B[39m \u001B[43m            \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    832\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    833\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtyp\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmanager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    834\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    836\u001B[39m \u001B[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001B[39;00m\n\u001B[32m    837\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m is_list_like(data):\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:336\u001B[39m, in \u001B[36mndarray_to_mgr\u001B[39m\u001B[34m(values, index, columns, dtype, copy, typ)\u001B[39m\n\u001B[32m    331\u001B[39m \u001B[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001B[39;00m\n\u001B[32m    332\u001B[39m index, columns = _get_axes(\n\u001B[32m    333\u001B[39m     values.shape[\u001B[32m0\u001B[39m], values.shape[\u001B[32m1\u001B[39m], index=index, columns=columns\n\u001B[32m    334\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m336\u001B[39m \u001B[43m_check_values_indices_shape_match\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    338\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m typ == \u001B[33m\"\u001B[39m\u001B[33marray\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    339\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28missubclass\u001B[39m(values.dtype.type, \u001B[38;5;28mstr\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:420\u001B[39m, in \u001B[36m_check_values_indices_shape_match\u001B[39m\u001B[34m(values, index, columns)\u001B[39m\n\u001B[32m    418\u001B[39m passed = values.shape\n\u001B[32m    419\u001B[39m implied = (\u001B[38;5;28mlen\u001B[39m(index), \u001B[38;5;28mlen\u001B[39m(columns))\n\u001B[32m--> \u001B[39m\u001B[32m420\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mShape of passed values is \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpassed\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, indices imply \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mimplied\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mValueError\u001B[39m: Shape of passed values is (4311, 60), indices imply (4311, 40)"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "80795772e4a87060"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
