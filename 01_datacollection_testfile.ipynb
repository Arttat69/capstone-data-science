{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Content for 01_data_collection.ipynb\n",
    "\n",
    "This notebook handles data collection, downloading raw data, basic processing, feature engineering, merging with GPR and news features, and saving enriched datasets.\n",
    "It saves processed, merged, and enriched CSVs to respective directories.\n",
    "Ensure you have kaggle CLI installed and configured for downloading datasets.\n"
   ],
   "id": "dfd7b87a3672196c"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-20T09:16:26.121347Z",
     "start_time": "2025-11-20T09:16:20.421461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Set paths\n",
    "ROOT = os.getcwd()\n",
    "DATA_DIR = os.path.join(ROOT, \"data\")\n",
    "RAW_DIR = os.path.join(DATA_DIR, \"raw\")\n",
    "PROCESSED_DIR = os.path.join(DATA_DIR, \"processed\")\n",
    "MERGED_DIR = os.path.join(DATA_DIR, \"merged\")\n",
    "ENRICHED_DIR = os.path.join(DATA_DIR, \"enriched\")\n",
    "\n",
    "for folder in [DATA_DIR, RAW_DIR, PROCESSED_DIR, MERGED_DIR, ENRICHED_DIR]:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# Commodity tickers and price columns\n",
    "tickers = {\n",
    "    \"Gold\": \"GC=F\",\n",
    "    \"WTI\": \"CL=F\",\n",
    "    \"Wheat\": \"ZW=F\",\n",
    "    \"NaturalGas\": \"UNG\",\n",
    "    \"Copper\": \"HG=F\",\n",
    "    \"Lithium\": \"LIT\"\n",
    "}\n",
    "price_cols = {\n",
    "    \"Gold\": \"Close_GC=F\",\n",
    "    \"WTI\": \"Close_CL=F\",\n",
    "    \"Wheat\": \"Close_ZW=F\",\n",
    "    \"NaturalGas\": \"Close_UNG\",\n",
    "    \"Copper\": \"Close_HG=F\",\n",
    "    \"Lithium\": \"Close_LIT\"\n",
    "}\n",
    "START = \"2000-01-01\"\n",
    "END = None\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\taton\\PycharmProjects\\Commodity project\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Part 2: Download Kaggle Datasets",
   "id": "1e7364e1e03fa849"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T09:16:32.812022Z",
     "start_time": "2025-11-20T09:16:26.204018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def download_kaggle(dataset, to_folder):\n",
    "    os.makedirs(to_folder, exist_ok=True)\n",
    "    subprocess.run([\n",
    "        \"kaggle\", \"datasets\", \"download\", \"-d\", dataset, \"-p\", to_folder, \"--unzip\"\n",
    "    ], check=True)\n",
    "    print(f\"Kaggle dataset {dataset} downloaded.\")\n",
    "\n",
    "# Download news dataset (ABC headlines)\n",
    "download_kaggle(\"therohk/million-headlines\", RAW_DIR)\n",
    "\n",
    "# Assume GPR dataset is downloaded manually or via Kaggle if available. For this example, assume it's placed in RAW_DIR/All_Historical_Data_Separately/\n",
    "# If GPR is from a Kaggle dataset, add download_kaggle here (e.g., if it's \"someuser/gpr-dataset\").\n",
    "\n",
    "# Functions for data processing\n",
    "def flatten_columns(df):\n",
    "    \"\"\"Flatten MultiIndex columns if needed.\"\"\"\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = ['_'.join(filter(None, map(str, col))).strip() for col in df.columns.values]\n",
    "    return df\n",
    "\n",
    "def drop_duplicate_dates(df, date_col):\n",
    "    # Remove rows with duplicated dates\n",
    "    df = df.drop_duplicates(subset=[date_col])\n",
    "    return df\n",
    "\n",
    "def download_commodity(ticker, name):\n",
    "    print(f\"Downloading {name} ({ticker}) ...\")\n",
    "    df = yf.download(ticker, start=START, end=END, auto_adjust=True)\n",
    "    if df.empty:\n",
    "        print(f\"Warning: Empty data for {name}\")\n",
    "        return pd.DataFrame()\n",
    "    df = flatten_columns(df)\n",
    "    df.reset_index(inplace=True)\n",
    "    fname = f\"{name.lower()}_raw.csv\"\n",
    "    df.to_csv(os.path.join(RAW_DIR, fname), index=False)\n",
    "    print(f\"Saved raw data: {fname}\")\n",
    "    return df\n",
    "\n",
    "def feature_engineer(df, price_col, name=\"Commodity\"):\n",
    "    \"\"\"\n",
    "    Feature engineering for commodity price DataFrame.\n",
    "    Computes return, moving average, volatility, and lagged returns.\n",
    "    Saves processed file to PROCESSED_DIR if name provided.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    if price_col not in df.columns:\n",
    "        print(f\"{name}: Price column '{price_col}' not found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = df.sort_values(\"Date\").drop_duplicates(\"Date\").reset_index(drop=True)\n",
    "    df[\"Return\"] = df[price_col].pct_change()\n",
    "    df[\"MA_5\"] = df[price_col].rolling(5).mean()\n",
    "    df[\"Vol_5\"] = df[\"Return\"].rolling(5).std()\n",
    "    df[\"Return_lag1\"] = df[\"Return\"].shift(1)\n",
    "    df[\"Return_lag10\"] = df[\"Return\"].shift(10)\n",
    "\n",
    "    # Drop rows with missing required fields\n",
    "    required_cols = [price_col, \"Return\", \"MA_5\", \"Vol_5\", \"Return_lag1\", \"Return_lag10\"]\n",
    "    df = df.dropna(subset=required_cols).reset_index(drop=True)\n",
    "\n",
    "    # Save processed data\n",
    "    fname = f\"{name.lower()}_processed.csv\"\n",
    "    save_path = os.path.join(PROCESSED_DIR, fname)\n",
    "    df.to_csv(save_path, index=False)\n",
    "    print(f\"Saved processed data: {save_path}\")\n",
    "\n",
    "    return df\n"
   ],
   "id": "89409aeccdb857",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle dataset therohk/million-headlines downloaded.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Download, Process Commodities\n",
   "id": "1da9483024a7c3e2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T09:16:38.039811Z",
     "start_time": "2025-11-20T09:16:33.452001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dfs_raw, dfs_proc = {}, {}\n",
    "for name, ticker in tickers.items():\n",
    "    df_raw = download_commodity(ticker, name)\n",
    "    dfs_raw[name] = df_raw\n",
    "    if not df_raw.empty:\n",
    "        dfs_proc[name] = feature_engineer(df_raw, price_cols[name], name)\n",
    "    else:\n",
    "        dfs_proc[name] = pd.DataFrame()\n",
    "# Load GPR\n",
    "def load_gpr(gpr_path):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses daily Geopolitical Risk Index data.\n",
    "    Resamples to daily frequency with forward fill.\n",
    "    Returns DataFrame with 'DATE', 'GPRD', 'GPRD_THREAT', and 'EVENT' columns if present.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(gpr_path):\n",
    "        print(f\"GPR dataset not found: {gpr_path}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    gpr = pd.read_csv(gpr_path)\n",
    "    if 'DATE' not in gpr.columns:\n",
    "        print(\"GPR dataset missing required 'DATE' column\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    gpr['DATE'] = pd.to_datetime(gpr['DATE'], errors='coerce')\n",
    "    gpr = gpr.dropna(subset=['DATE'])\n",
    "    gpr = gpr.drop_duplicates(subset=['DATE'])\n",
    "    gpr = gpr.sort_values('DATE').reset_index(drop=True)\n",
    "\n",
    "    # Resample to daily frequency and forward-fill missing dates\n",
    "    gpr_daily = gpr.set_index('DATE').resample('D').ffill().reset_index()\n",
    "\n",
    "    # Select only relevant columns\n",
    "    keep_cols = [col for col in ['DATE', 'GPRD', 'GPRD_THREAT', 'EVENT'] if col in gpr_daily.columns]\n",
    "    gpr_daily = gpr_daily[keep_cols]\n",
    "\n",
    "    # Rename DATE column to 'Date' for consistency\n",
    "    gpr_daily = gpr_daily.rename(columns={'DATE':'Date'})\n",
    "\n",
    "    return gpr_daily\n",
    "\n",
    "# Merge with GPR\n",
    "def merge_with_gpr(df, gpr_df, name):\n",
    "    if df.empty or gpr_df.empty:\n",
    "        print(f\"Skipping merge for {name}: Empty dataframe(s).\")\n",
    "        return pd.DataFrame()\n",
    "    df = drop_duplicate_dates(df, \"Date\")\n",
    "    merged = pd.merge(df, gpr_df, left_on=\"Date\", right_on=\"Date\", how=\"left\")\n",
    "    fname = f\"{name.lower()}_merged.csv\"\n",
    "    merged.to_csv(os.path.join(MERGED_DIR, fname), index=False)\n",
    "    print(f\"Saved merged data: {fname}\")\n",
    "    return merged\n",
    "\n",
    "gpr_path = os.path.join(RAW_DIR, \"All_Historical_Data_Separately\", \"Geopolitical Risk Index Daily.csv\")  # Adjust if needed\n",
    "gpr_daily = load_gpr(gpr_path)\n",
    "dfs_merged = {}\n",
    "for name, df_proc in dfs_proc.items():\n",
    "    dfs_merged[name] = merge_with_gpr(df_proc, gpr_daily, name)"
   ],
   "id": "cdc8097ce6b411f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Gold (GC=F) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved raw data: gold_raw.csv\n",
      "Saved processed data: C:\\Users\\taton\\PycharmProjects\\Commodity project\\data\\processed\\gold_processed.csv\n",
      "Downloading WTI (CL=F) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved raw data: wti_raw.csv\n",
      "Saved processed data: C:\\Users\\taton\\PycharmProjects\\Commodity project\\data\\processed\\wti_processed.csv\n",
      "Downloading Wheat (ZW=F) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved raw data: wheat_raw.csv\n",
      "Saved processed data: C:\\Users\\taton\\PycharmProjects\\Commodity project\\data\\processed\\wheat_processed.csv\n",
      "Downloading NaturalGas (UNG) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved raw data: naturalgas_raw.csv\n",
      "Saved processed data: C:\\Users\\taton\\PycharmProjects\\Commodity project\\data\\processed\\naturalgas_processed.csv\n",
      "Downloading Copper (HG=F) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved raw data: copper_raw.csv\n",
      "Saved processed data: C:\\Users\\taton\\PycharmProjects\\Commodity project\\data\\processed\\copper_processed.csv\n",
      "Downloading Lithium (LIT) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved raw data: lithium_raw.csv\n",
      "Saved processed data: C:\\Users\\taton\\PycharmProjects\\Commodity project\\data\\processed\\lithium_processed.csv\n",
      "Saved merged data: gold_merged.csv\n",
      "Saved merged data: wti_merged.csv\n",
      "Saved merged data: wheat_merged.csv\n",
      "Saved merged data: naturalgas_merged.csv\n",
      "Saved merged data: copper_merged.csv\n",
      "Saved merged data: lithium_merged.csv\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load and process news",
   "id": "c65ecc2a8e76b6f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T09:20:51.025331Z",
     "start_time": "2025-11-20T09:16:38.130617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "news_df = pd.read_csv(os.path.join(RAW_DIR, \"abcnews-date-text.csv\"))\n",
    "news_df['date'] = pd.to_datetime(news_df['publish_date'].astype(str), format='%Y%m%d', errors='coerce')\n",
    "\n",
    "# Geopolitical keywords\n",
    "geo_keywords = [\n",
    "    'war', 'wars', 'sanctions', 'sanction', 'conflict', 'conflicts', 'geopolitical', 'tension', 'tensions',\n",
    "    'embargo', 'embargoes', 'crisis', 'crises', 'invasion', 'invasions', 'terrorism', 'opec', 'blockade',\n",
    "    'blockades', 'dispute', 'disputes', 'escalation', 'escalations', 'hostility', 'hostilities', 'unrest',\n",
    "    'strike', 'strikes', 'alliance', 'alliances', 'treaty', 'treaties', 'summit', 'summits', 'diplomacy',\n",
    "    'iran', 'syria', 'syrian', 'libya', 'lybian', 'iraq', 'north korea', 'ukraine', 'russia', 'china', 'trade war', 'trade wars',\n",
    "    'missile', 'missiles', 'military', 'nuclear', 'sanctioned', 'ceasefire', 'ceasefires', 'negotiation',\n",
    "    'negotiations', 'occupation', 'occupations', 'regime', 'regimes', 'rebel', 'rebels', 'protest', 'protests',\n",
    "    'cyberattack', 'cyberattacks', 'espionage', 'border', 'borders', 'refugee', 'refugees', 'intervention',\n",
    "    'interventions', 'pipeline', 'pipelines', 'tariff', 'tariffs', 'boycott', 'boycotts', 'expulsion',\n",
    "    'expulsions', 'diplomat', 'diplomats', 'embassy', 'embassies', 'coalition', 'coalitions', 'genocide',\n",
    "    'genocides', 'hostage', 'hostages', 'radical', 'radicals', 'siege', 'sieges', 'nato', 'chechen', 'lebanon', 'yemen', 'taliban', 'islamist', 'afghanistan', 'kabul', 'saddam hussein', 'global financial crisis', 'economic recession', 'economic recessions', 'chaos', 'unemployment', 'instability','insolvency', 'credit crunch', 'unpayable debts', 'abkhazia', 'ossetia', 'separatists', 'donetsk', 'luhansk', 'south sudan', 'jihadism', 'palestine', 'palestinian', 'isis', 'crimea', 'annexation', 'houthi', 'migration crises', 'donbas', 'arab spring', 'kosovo', 'ukraine war', 'ukraine conflict', 'AFU'\n",
    "]\n",
    "\n",
    "def extract_news_features_abc(news_df, keywords):\n",
    "    news_df = news_df.copy()\n",
    "    news_df['geo_keyword_hits'] = news_df['headline_text'].apply(\n",
    "        lambda text: sum(kw in text for kw in keywords if isinstance(text, str))\n",
    "    )\n",
    "    news_df['sentiment'] = news_df['headline_text'].apply(\n",
    "        lambda x: TextBlob(x).sentiment.polarity if isinstance(x, str) else 0\n",
    "    )\n",
    "    news_daily = news_df.groupby('date').agg({\n",
    "        'geo_keyword_hits': 'sum',\n",
    "        'sentiment': 'mean'\n",
    "    }).reset_index().dropna(subset=['date']).sort_values('date')\n",
    "    return news_daily\n",
    "\n",
    "news_features = extract_news_features_abc(news_df, geo_keywords)\n",
    "news_features = news_features.rename(columns={'date': 'Date'})\n",
    "news_features['Date'] = pd.to_datetime(news_features['Date'])\n",
    "\n",
    "# Enrich with news features\n",
    "merged_data = {}\n",
    "gpr_daily['Date'] = pd.to_datetime(gpr_daily['Date'])\n",
    "\n",
    "for name, ticker in tickers.items():\n",
    "    price_col = price_cols[name]\n",
    "    df_price = dfs_raw.get(name, pd.DataFrame())  # Use raw or processed? Using processed for features\n",
    "    if df_price.empty:\n",
    "        continue\n",
    "    df_feat = dfs_proc.get(name, pd.DataFrame())\n",
    "\n",
    "    df_feat['Date'] = pd.to_datetime(df_feat['Date'])\n",
    "\n",
    "    # Merge with GPR and news using merge_asof\n",
    "    df_merge = pd.merge_asof(df_feat, gpr_daily, on='Date', direction='backward')\n",
    "    df_merge = pd.merge_asof(df_merge, news_features, on='Date', direction='backward')\n",
    "\n",
    "    # Event dummy\n",
    "    if 'EVENT' in df_merge.columns:\n",
    "        df_merge['event_dummy'] = df_merge['EVENT'].notna().astype(int)\n",
    "    else:\n",
    "        df_merge['event_dummy'] = 0\n",
    "\n",
    "    # Save enriched\n",
    "    fname = f\"{name.lower()}_enriched.csv\"\n",
    "    df_merge.to_csv(os.path.join(ENRICHED_DIR, fname), index=False)\n",
    "    merged_data[name] = df_merge\n",
    "\n",
    "# Trim to end date if needed (from code)\n",
    "end_date = pd.to_datetime(\"2021-12-31\")\n",
    "for name, df in merged_data.items():\n",
    "    merged_data[name] = df[df['Date'] <= end_date].reset_index(drop=True)\n",
    "    # Overwrite enriched with trimmed\n",
    "    fname = f\"{name.lower()}_enriched.csv\"\n",
    "    merged_data[name].to_csv(os.path.join(ENRICHED_DIR, fname), index=False)\n",
    "\n",
    "print(\"Data collection complete. Enriched datasets saved to\", ENRICHED_DIR)"
   ],
   "id": "32f5170968590a6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collection complete. Enriched datasets saved to C:\\Users\\taton\\PycharmProjects\\Commodity project\\data\\enriched\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
