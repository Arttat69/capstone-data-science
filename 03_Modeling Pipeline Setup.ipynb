{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-08T19:23:06.172601Z",
     "start_time": "2025-11-08T19:23:06.137427Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Set paths\n",
    "ROOT = os.getcwd()\n",
    "DATA_DIR = os.path.join(ROOT, \"data\")\n",
    "RAW_DIR = os.path.join(DATA_DIR, \"raw\")\n",
    "PROCESSED_DIR = os.path.join(DATA_DIR, \"processed\")\n",
    "MERGED_DIR = os.path.join(DATA_DIR, \"merged\")\n",
    "ENRICHED_DIR  = os.path.join(DATA_DIR, \"enriched\")\n",
    "\n",
    "for folder in [DATA_DIR, RAW_DIR, PROCESSED_DIR, MERGED_DIR, ENRICHED_DIR]:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T19:23:07.251861Z",
     "start_time": "2025-11-08T19:23:07.232592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_gpr():\n",
    "    \"\"\"\n",
    "    Loads and preprocesses Geopolitical Risk Index daily data.\n",
    "    Returns a DataFrame indexed by daily dates with forward-filled values.\n",
    "    \"\"\"\n",
    "    gpr_path = os.path.join(RAW_DIR, \"All_Historical_Data_Separately\", \"Geopolitical Risk Index Daily.csv\")\n",
    "    if not os.path.exists(gpr_path):\n",
    "        print(f\"GPR dataset not found: {gpr_path}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    gpr = pd.read_csv(gpr_path)\n",
    "    if 'DATE' not in gpr.columns:\n",
    "        print(\"GPR dataset missing required 'DATE' column\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    gpr['DATE'] = pd.to_datetime(gpr['DATE'], errors='coerce')\n",
    "    gpr = gpr.dropna(subset=['DATE'])\n",
    "    gpr = gpr.drop_duplicates(subset=['DATE']).sort_values('DATE').reset_index(drop=True)\n",
    "\n",
    "    # Resample daily and forward-fill missing dates\n",
    "    gpr_daily = gpr.set_index('DATE').resample('D').ffill().reset_index()\n",
    "\n",
    "    # Keep only relevant columns if present\n",
    "    keep_cols = [col for col in ['DATE', 'GPRD', 'GPRD_THREAT', 'EVENT'] if col in gpr_daily.columns]\n",
    "    return gpr_daily[keep_cols].rename(columns={'DATE':'Date'})\n",
    "\n",
    "def extract_news_features(news_df, keywords):\n",
    "    \"\"\"\n",
    "    Extracts daily aggregated geopolitical keyword hits and sentiment polarity from news headlines.\n",
    "    Args:\n",
    "        news_df (DataFrame): Raw news data with 'headline_text' and 'date' columns.\n",
    "        keywords (list of str): List of geopolitical keywords to count in headlines.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: daily aggregated features with columns ['Date', 'geo_keyword_hits', 'sentiment']\n",
    "    \"\"\"\n",
    "    news_df = news_df.copy()\n",
    "\n",
    "    # Count geopolitics keyword hits in each headline\n",
    "    news_df['geo_keyword_hits'] = news_df['headline_text'].apply(\n",
    "        lambda text: sum(kw in text.lower() for kw in keywords) if isinstance(text, str) else 0\n",
    "    )\n",
    "\n",
    "    # Calculate sentiment polarity of each headline's text (TextBlob)\n",
    "    news_df['sentiment'] = news_df['headline_text'].apply(\n",
    "        lambda x: TextBlob(x).sentiment.polarity if isinstance(x, str) else 0\n",
    "    )\n",
    "\n",
    "    # Aggregate daily by summing keyword hits and averaging sentiment\n",
    "    news_daily = news_df.groupby('date').agg({\n",
    "        'geo_keyword_hits': 'sum',\n",
    "        'sentiment': 'mean'\n",
    "    }).reset_index().dropna(subset=['date']).sort_values('date')\n",
    "\n",
    "    return news_daily.rename(columns={'date':'Date'})"
   ],
   "id": "889940d9a56220e7",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T19:23:09.298594Z",
     "start_time": "2025-11-08T19:23:08.949963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assuming gpr_daily and news_features are loaded and preprocessed by your defined functions:\n",
    "# Example: load raw news data CSV into news_df\n",
    "abcnews_path = os.path.join(RAW_DIR, \"abcnews-date-text.csv\")\n",
    "news_df = pd.read_csv(news_path)\n",
    "\n",
    "# Proceed to extract features\n",
    "news_features = extract_news_features(news_df, geo_keywords)\n",
    "\n",
    "gpr_daily = load_gpr()\n",
    "print(gpr_daily.columns)\n",
    "print(gpr_daily['Date'].dtype)\n",
    "\n",
    "news_features = extract_news_features(news_df, geo_keywords)\n",
    "\n",
    "\n",
    "print(news_features.columns)\n",
    "print(news_features['Date'].dtype)\n",
    "\n"
   ],
   "id": "cbd53ecc917b381f",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\taton\\\\PycharmProjects\\\\Commodity project\\\\data\\\\raw\\\\All_Historical_Data_Separately\\\\raw_news_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[25]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Assuming gpr_daily and news_features are loaded and preprocessed by your defined functions:\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[38;5;66;03m# Example: load raw news data CSV into news_df\u001B[39;00m\n\u001B[32m      3\u001B[39m abcnews_path = os.path.join(RAW_DIR, \u001B[33m\"\u001B[39m\u001B[33mabcnews-date-text.csv\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m news_df = \u001B[43mpd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnews_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[38;5;66;03m# Proceed to extract features\u001B[39;00m\n\u001B[32m      7\u001B[39m news_features = extract_news_features(news_df, geo_keywords)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001B[39m, in \u001B[36mread_csv\u001B[39m\u001B[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[39m\n\u001B[32m   1013\u001B[39m kwds_defaults = _refine_defaults_read(\n\u001B[32m   1014\u001B[39m     dialect,\n\u001B[32m   1015\u001B[39m     delimiter,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1022\u001B[39m     dtype_backend=dtype_backend,\n\u001B[32m   1023\u001B[39m )\n\u001B[32m   1024\u001B[39m kwds.update(kwds_defaults)\n\u001B[32m-> \u001B[39m\u001B[32m1026\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001B[39m, in \u001B[36m_read\u001B[39m\u001B[34m(filepath_or_buffer, kwds)\u001B[39m\n\u001B[32m    617\u001B[39m _validate_names(kwds.get(\u001B[33m\"\u001B[39m\u001B[33mnames\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[32m    619\u001B[39m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m620\u001B[39m parser = \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    622\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[32m    623\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001B[39m, in \u001B[36mTextFileReader.__init__\u001B[39m\u001B[34m(self, f, engine, **kwds)\u001B[39m\n\u001B[32m   1617\u001B[39m     \u001B[38;5;28mself\u001B[39m.options[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m] = kwds[\u001B[33m\"\u001B[39m\u001B[33mhas_index_names\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m   1619\u001B[39m \u001B[38;5;28mself\u001B[39m.handles: IOHandles | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1620\u001B[39m \u001B[38;5;28mself\u001B[39m._engine = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001B[39m, in \u001B[36mTextFileReader._make_engine\u001B[39m\u001B[34m(self, f, engine)\u001B[39m\n\u001B[32m   1878\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[32m   1879\u001B[39m         mode += \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1880\u001B[39m \u001B[38;5;28mself\u001B[39m.handles = \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1881\u001B[39m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1882\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1883\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mencoding\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1884\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcompression\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1885\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmemory_map\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1886\u001B[39m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[43m=\u001B[49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1887\u001B[39m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mencoding_errors\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstrict\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1888\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstorage_options\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1889\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1890\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m.handles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1891\u001B[39m f = \u001B[38;5;28mself\u001B[39m.handles.handle\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python\\Lib\\site-packages\\pandas\\io\\common.py:873\u001B[39m, in \u001B[36mget_handle\u001B[39m\u001B[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[39m\n\u001B[32m    868\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m    869\u001B[39m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[32m    870\u001B[39m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[32m    871\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m ioargs.encoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs.mode:\n\u001B[32m    872\u001B[39m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m873\u001B[39m         handle = \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[32m    874\u001B[39m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    875\u001B[39m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    876\u001B[39m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m=\u001B[49m\u001B[43mioargs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    877\u001B[39m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[43m=\u001B[49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    878\u001B[39m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    879\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    880\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    881\u001B[39m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[32m    882\u001B[39m         handle = \u001B[38;5;28mopen\u001B[39m(handle, ioargs.mode)\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\taton\\\\PycharmProjects\\\\Commodity project\\\\data\\\\raw\\\\All_Historical_Data_Separately\\\\raw_news_data.csv'"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T01:54:36.309988Z",
     "start_time": "2025-11-09T01:54:36.268278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ],
   "id": "8696dcdc1c726eb5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.20.0\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load or define tickers and price_cols dictionaries as per your project\n",
    "\n",
    "merged_data = {}\n",
    "\n",
    "for name, ticker in tickers.items():\n",
    "    price_col = price_cols[name]\n",
    "    df_price = download_commodity(ticker, name)\n",
    "    if df_price.empty:\n",
    "        continue\n",
    "    df_feat = feature_engineer(df_price, price_col)\n",
    "\n",
    "    df_feat['Date'] = pd.to_datetime(df_feat['Date'])\n",
    "    df_feat = df_feat.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "    df_merge = pd.merge_asof(df_feat, gpr_daily, on='Date', direction='backward')\n",
    "    df_merge = pd.merge_asof(df_merge, news_features, on='Date', direction='backward')\n",
    "\n",
    "    if 'EVENT' in df_merge.columns:\n",
    "        df_merge['event_dummy'] = df_merge['EVENT'].notna().astype(int)\n",
    "    else:\n",
    "        df_merge['event_dummy'] = 0\n",
    "\n",
    "    # Save enriched data\n",
    "    fname = f\"{name.lower()}_enriched.csv\"\n",
    "    df_merge.to_csv(os.path.join(ENRICHED_DIR, fname), index=False)\n",
    "\n",
    "    merged_data[name] = df_merge"
   ],
   "id": "de137cf685b0048a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Now modeling pipeline setup\n",
    "\n",
    "def prepare_features_targets(df, features, target):\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    return X, y\n",
    "\n",
    "# Example modeling for one commodity (Gold)\n",
    "commodity = 'Gold'\n",
    "df = merged_data[commodity]\n",
    "\n",
    "# Select features and target\n",
    "features_baseline = ['Return_lag1']\n",
    "features_enhanced = ['Return_lag1', 'GPRD', 'geo_keyword_hits', 'sentiment', 'event_dummy']\n",
    "target = 'Return'\n",
    "\n",
    "# Split train and test sets by date (no random shuffle since time series data)\n",
    "split_date = pd.to_datetime('2019-01-01')\n",
    "train_df = df[df['Date'] < split_date]\n",
    "test_df = df[df['Date'] >= split_date]\n",
    "\n",
    "# Prepare data\n",
    "X_train_base, y_train = prepare_features_targets(train_df, features_baseline, target)\n",
    "X_test_base, y_test = prepare_features_targets(test_df, features_baseline, target)\n",
    "\n",
    "X_train_enh, _ = prepare_features_targets(train_df, features_enhanced, target)\n",
    "X_test_enh, _ = prepare_features_targets(test_df, features_enhanced, target)\n",
    "\n",
    "# Scaling (important for models sensitive to scale)\n",
    "scaler = StandardScaler()\n",
    "X_train_enh_scaled = scaler.fit_transform(X_train_enh)\n",
    "X_test_enh_scaled = scaler.transform(X_test_enh)\n"
   ],
   "id": "74c09c801f0991bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Baseline Linear Regression\n",
    "lr_base = LinearRegression()\n",
    "lr_base.fit(X_train_base, y_train)\n",
    "y_pred_base = lr_base.predict(X_test_base)\n",
    "print(f\"Baseline model RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_base)):.4f}\")\n",
    "\n",
    "# Enhanced Random Forest Regression\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_enh_scaled, y_train)\n",
    "y_pred_rf = rf.predict(X_test_enh_scaled)\n",
    "print(f\"Enhanced model RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_rf)):.4f}\")\n"
   ],
   "id": "198783090820f8b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Classification: Up/Down movement\n",
    "train_df['Return_binary'] = (train_df['Return'] > 0).astype(int)\n",
    "test_df['Return_binary'] = (test_df['Return'] > 0).astype(int)\n",
    "\n",
    "X_train_class = scaler.fit_transform(train_df[features_enhanced])\n",
    "X_test_class = scaler.transform(test_df[features_enhanced])\n",
    "y_train_class = train_df['Return_binary']\n",
    "y_test_class = test_df['Return_binary']\n",
    "\n",
    "logreg = LogisticRegression(max_iter=200)\n",
    "logreg.fit(X_train_class, y_train_class)\n",
    "y_pred_class = logreg.predict(X_test_class)\n",
    "print(f\"Classification Accuracy: {accuracy_score(y_test_class, y_pred_class):.4f}\")\n",
    "\n",
    "# Clustering for regime detection\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "regime_features = df[['Vol_5', 'GPRD', 'geo_keyword_hits']].fillna(0)\n",
    "df['Regime'] = kmeans.fit_predict(regime_features)"
   ],
   "id": "5411c7c77adbd2fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Optional LSTM for sequence forecasting\n",
    "def create_sequences(data, feature_cols, target_col, seq_length=10):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[feature_cols].iloc[i:(i + seq_length)].values\n",
    "        y = data[target_col].iloc[i + seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "seq_length = 10\n",
    "feature_cols = features_enhanced\n",
    "target_col = target\n",
    "\n",
    "X_seq, y_seq = create_sequences(df.reset_index(drop=True), feature_cols, target_col, seq_length)\n",
    "\n",
    "split_idx = len(train_df) - seq_length  # Adjust split for sequences\n",
    "X_train_seq, y_train_seq = X_seq[:split_idx], y_seq[:split_idx]\n",
    "X_test_seq, y_test_seq = X_seq[split_idx:], y_seq[split_idx:]\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(50, input_shape=(seq_length, len(feature_cols))),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(X_train_seq, y_train_seq, epochs=10, batch_size=32, verbose=2)\n",
    "\n",
    "y_pred_lstm = model.predict(X_test_seq)\n",
    "print(f\"LSTM Test RMSE: {np.sqrt(mean_squared_error(y_test_seq, y_pred_lstm)):.4f}\")"
   ],
   "id": "5c7966a250e51bd5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
